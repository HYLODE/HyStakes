{
  "hash": "00fb80a57cdf9a8c49d3ddef7bb25584",
  "result": {
    "markdown": "---\ntitle: Predicting discharge using Bayesian statistics\nformat:\n  html:\n    code-fold: true\n---\n\nOur goal is to develop a Bayesian model to predict a patient's probability of being discharged in the next 24 hours. \n\nNote that, for now in this first attempt, I'm assuming you have some prior knowledge of Bayesian statistics. \n\nHere are three initial steps I think we need to do: \n\n 1. Decide on our \"prior\" probability and express it as a distribution \n 2. Identify some data we can use for the \"likelihood\". This can be synthetic to start with\n 3. Develop a posterior distribution from the prior and likelihood. \n\n### 1. Prior probability\n\nWe express the probability of discharge as $\\theta$. Our prior belief about $\\theta$ could be that it follows a beta distribution which is expressed as follows:\n\n$$\nP(\\theta) = Beta(\\alpha,\\beta)\n$$ {#eq-prior}\n\nLet's say we know that 5% of patients are discharged on any given day. Therefore we'd like the expected value of our Beta distribution to be at 0.05. The expected value of a Beta distribution is given by\n\n$$\nExp(\\theta) = \\frac{\\alpha}{\\alpha + \\beta}\n$$\n\nIf we set $\\alpha$ to be 2 and $\\beta$ to be 38, we'll get an expected value of 0.05\n\n$$\nP(\\theta) = Beta(2,38)\n$$ {#eq-prior2}\n\nPlotting this: \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n\na, b = 2, 38\nfig, ax = plt.subplots(1, 1)\n\n#x = np.linspace(beta.ppf(0.01, a, b), beta.ppf(0.99, a, b), 100)\ntheta = np.linspace(0, 1, 100)\nax.plot(theta, beta.pdf(theta, a, b),\n       'r-', lw=2, alpha=0.6, label='beta pdf')\nax.set_ylabel('probability density function')\nax.set_xlabel('theta = probability of discharge in 24 hours')\nax.set_title('Prior belief about distribution of theta')\nplt.xlim([0, 1])\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Prior distribution](bayes-for-discharge_files/figure-html/prior-output-1.png){#prior width=595 height=449}\n:::\n:::\n\n\n### 2. Identify some data we can use for the \"likelihood\"\n\nNow let's look at some data. This will constitute our likelihood. First let's read it in from a saved anonymised database\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport sqlalchemy as sa\nimport pandas as pd\nimport os\n\n#os.chdir('/Users/zellaking/Repos/HyStakes/hystakes/icu_discharges')\n\n# achieved this only by changing directory\nsqlite_engine = sa.create_engine('sqlite:///../../data/dummy.db')\n\n#df = pd.read_sql_table(    'discharges', con=sqlite_engine)\n\ndf = pd.read_sql_query(\"SELECT * from discharges\", sqlite_engine)\ndf.head()\n\n# Let N be the number of patients observed\nN = df.shape[0]\n\n# Let X be the number of patients who were discharged in 24 hours\nX = df[df.hours_to_discharge <= 24].shape[0]\nprint(\"Number of people discharged in 24 hours: \" + str(X))\nprint(\"Total number of people observed: \" + str(N))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of people discharged in 24 hours: 52\nTotal number of people observed: 444\n```\n:::\n:::\n\n\nWe observe that 52 patients out of 444 were discharged. This provides our likelihood. We can express our data as a binomial distribution where we do N trials with our unknown probability $\\theta$ and return the result we are looking for (ie patient was discharged) X times of the N draws. \n\n$$\nP(data) = Binomial(N, \\theta)\n$$ {#eq-likelihood}\n\nThe binomial distribution can be expanded as shown here, where ${N \\choose X}$ is the notation for N Choose X (a way of expressing the number of different ways, or permutations, that you could get a result of X discharged patients when starting with N patients in total) \n\n$$\nP(data) = {N \\choose X} \\theta^x(1-\\theta)^{(N-X)}\n$$\n\nWe can plot the likelihood\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport os\n\n#| label: likelihood\n#| fig-cap: \"Likelihood distribution\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\nfrom math import factorial as fact\nN_choose_X = fact(N)/(fact(X)*fact(N-X))\n\nfig, ax = plt.subplots(1, 1)\n\n\nline1, = ax.plot(theta, beta.pdf(theta, a, b),\n       'r-', lw=2, alpha=0.6, label='prior')\nline2, = ax.plot(theta, N_choose_X*theta**X*(1-theta)**(N-X)*N,\n       'g-', lw=2, alpha=0.6, label='likelihood')\nax.legend(handles=[line1, line2])\nax.set_ylabel('probability density function')\nax.set_xlabel('theta = probability of discharge in 24 hours')\nax.set_title('Prior belief about distribution of theta, and likelihood of data')\n\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](bayes-for-discharge_files/figure-html/cell-4-output-1.png){width=585 height=449}\n:::\n:::\n\n\nNow that we have seen the data, we see that our original prior was too negative. More patients are discharged each day than we expected. \n\n### 3. Develop a posterior distribution from the prior and likelihood\n\nWe know from Bayesian statistics that, if you have a beta prior, and binomial likelihood then you can derive the posterior distribution from these two without doing any complicated maths. \n\nSpecifically, our posterior can be expressed as a beta distribution, in which we can bring in the values of $\\theta$ from our prior, and values of N and X from our likelihood\n\n$$\nP(\\theta | data) = Beta(\\alpha + X,N + \\beta - X)\n$$\n\nAnd this can also be expressed as\n\n$$\nP(\\theta | data) = \\frac{\\theta^{(\\alpha + X -1)}(1 - \\theta)^{N + \\beta - X -1}}{B (\\alpha + X -1, N + \\beta - X -1)}\n$$\n The denominator above is referred to as a normalising constant. It looks confusing, but the key thing to note that it doesn't contain any values of $\\theta$ making it simpler to work out than when you have to integrate over all values of $\\theta$. \n\n We can now draw our posterior distribution (in blue) \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n\na_prime, b_prime = 2 + X , 38 + N - X\nfig, ax = plt.subplots(1, 1)\n\n\nline1, = ax.plot(theta, beta.pdf(theta, a, b),\n       'r-', lw=2, alpha=0.6, label='prior')\nline2, = ax.plot(theta, N_choose_X*theta**X*(1-theta)**(N-X)*N,\n       'g-', lw=2, alpha=0.6, label='likelihood')\nline3, = ax.plot(theta, beta.pdf(theta, a_prime, b_prime),\n       'b-', lw=2, alpha=0.6, label='posterior')\nax.legend(handles=[line1, line2, line3])\n\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Posterior distribution](bayes-for-discharge_files/figure-html/posterior-output-1.png){#posterior width=566 height=411}\n:::\n:::\n\n\nThe posterior predictive distribution is in a tighter range (a taller, slimmer peak), and is overwhelmingly determined by the data, rather than the prior. This is the case with Bayesian statistics: as you get more data, the prior beliefs become less important.  \n\n",
    "supporting": [
      "bayes-for-discharge_files"
    ],
    "filters": [],
    "includes": {}
  }
}