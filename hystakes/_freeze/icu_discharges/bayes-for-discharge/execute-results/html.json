{
  "hash": "4c89a3cded25062d3199ddb05724a230",
  "result": {
    "markdown": "---\ntitle: Predicting discharge using Bayesian statistics\nformat:\n  html:\n    code-fold: true\n---\n\nOur goal is to develop a Bayesian model to predict a patient's probability of being discharged in the next 24 hours. \n\nNote that, for now in this first attempt, I'm assuming you have some prior knowledge of Bayesian statistics. \n\nHere are three initial steps I think we need to do: \n\n 1. Decide on our \"prior\" probability and express it as a distribution \n 2. Identify some data we can use for the \"likelihood\". This can be synthetic to start with\n 3. Develop a posterior distribution from the prior and likelihood. \n\n# 1. Prior probability\n\nWe expressed the probability of discharge as $\\theta$. Our prior belief about \\theta could be that it follows a beta distribution which is expressed as follows:\n\n$$\n\\theta = Beta(\\alpha,\\beta)\n$$ {#eq-prior}\n\nLet's say we know that 5% of patients are discharged on any given day. Therefore we'd like the expected value of our Beta distribution to be at 0.05. The expected value of a Beta distribution is given by\n\n$$\nExp(\\theta) = \\frac{\\alpha}{\\alpha + \\beta}\n$$\n\nSo if we set \\alpha to be 1 and \\beta to be 19, we'll get an expected value of 0.05\n\n$$\n\\theta = Beta(1,19)\n$$ {#eq-prior2}\n\nPlotting this: \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n\na, b = 2, 38\nfig, ax = plt.subplots(1, 1)\n\n#x = np.linspace(beta.ppf(0.01, a, b), beta.ppf(0.99, a, b), 100)\nx = np.linspace(0, 1, 100)\nprint(x)\nax.plot(x, beta.pdf(x, a, b),\n       'r-', lw=2, alpha=0.6, label='beta pdf')\nplt.xlim([0, 1])\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111\n 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323\n 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n 0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n 0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141\n 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596\n 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n 0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172\n 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n 0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384\n 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899\n 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n 0.96969697 0.97979798 0.98989899 1.        ]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Prior distribution](bayes-for-discharge_files/figure-html/prior-output-2.png){#prior width=577 height=411}\n:::\n:::\n\n\n# 2. Identify some data we can use for the \"likelihood\"\n\nNow let's look at some data. This will constitute our likelihood. First let's read it in from a saved anonymised database\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport sqlalchemy as sa\nimport pandas as pd\nimport os\n\n#os.chdir('/Users/zellaking/Repos/HyStakes/hystakes/icu_discharges')\n\n# achieved this only by changing directory\nsqlite_engine = sa.create_engine('sqlite:///../../data/dummy.db')\n\n#df = pd.read_sql_table(    'discharges', con=sqlite_engine)\n\ndf = pd.read_sql_query(\"SELECT * from discharges\", sqlite_engine)\ndf.head()\n\n# Let N be the number of patients observed\nN = df.shape[0]\n\n# Let X be the number of patients who were discharged in 24 hours\nX = df[df.hours_to_discharge <= 24].shape[0]\nprint(\"Number of people discharged in 24 hours: \" + str(X))\nprint(\"Total number of people observed: \" + str(N))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of people discharged in 24 hours: 52\nTotal number of people observed: 444\n```\n:::\n:::\n\n\nWe observe that X patients out of N were discharged. This provides our likelihood. We can express this as a binomial distribution as follows where {N \\choose X} is the notation for N Choose X (a way of expressing the number of different ways, or permutations, that you could get a result of X discharged patients when starting with N patients in total) \n\n$$\n{N \\choose X} \\theta^x(1-\\theta)^{(N-X)}\n$$\n\n# 3. Develop a posterior distribution from the prior and likelihood\n\nWe know from Bayesian statistics that, if you have a beta prior, and binomial likelihood then you can derive the posterior distribution from these two without doing any complicated maths. \n\nSpecifically, our posterior can be expressed as a beta distribution, in which we can bring in the values of \\theta from our prior, and values of N and X from our likelihood\n\n$$\nP(\\theta | data) = Beta(\\alpha + X,N + \\beta - X)\n$$\n\nAnd this can also be expressed like this\n\n$$\nP(\\theta | data) = \\frac{\\theta^{(\\alpha + X -1)}(1 - \\theta)^{N + \\beta - X -1}}{B (\\alpha + X -1, N + \\beta - X -1)}\n$$\n The denominator above is referred to as a normalising constant. It looks confusing, but the key thing to note that it doesn't contain any values of \\theta making it simpler to work out than when you have to integrate over all values of \\theta. \n\n We can now draw our posterior distribution. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n\na_prime, b_prime = 2 + X , 38 + N - X\nfig, ax = plt.subplots(1, 1)\n\nax.plot(x, beta.pdf(x, a, b),\n       'r-', lw=2, alpha=0.6, label='beta pdf')\nax.plot(x, beta.pdf(x, a_prime, b_prime),\n       'b-', lw=2, alpha=0.6, label='beta pdf')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Posterior distribution](bayes-for-discharge_files/figure-html/posterior-output-1.png){#posterior width=566 height=411}\n:::\n:::\n\n\nNow that we have seen the data, we see that our original prior was too negative. More patients are discharged each day than we expected. The probability is in a tighter range (a taller, slimmer peak), because we have observed {{N}} or md(f\"{n}) patients.  \n\n",
    "supporting": [
      "bayes-for-discharge_files"
    ],
    "filters": [],
    "includes": {}
  }
}