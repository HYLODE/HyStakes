[
  {
    "objectID": "vignettes/estimate-discharges.html",
    "href": "vignettes/estimate-discharges.html",
    "title": "Estimating discharge probability",
    "section": "",
    "text": "Using a grid approximation\nThis example uses a grid approximation to estimate the posterior. It all works by hand with nothing more than arithmetic.\nWe should set this up as a real ‘question’: that is please review the planned discharges from yesterday and send an alert out if you think didn’t get the discharges we’re expecting.\nTry to set up a task that does not depend on use of a probability distribution, or any prior knowledge. Attempt to adapt the worked example from Think Bayes (Chapter 4: Estimating proportions).\nAfter this then introduce the binomial as a way of making the counting step more efficient (i.e. “n choose k”)\n\n\nCode\nimport numpy as np\nfrom scipy.stats import binom\nfrom empiricaldist import Pmf\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nn = 2\np = 0.5\nk =1\nbinom.pmf(k, n, p)\n\n\n\n\nCode\nks = np.arange(n+1)\nps = binom.pmf(ks, n, p)\nps\n\n\narray([0.25, 0.5 , 0.25])\n\n\n\n\nCode\ndef make_binomial(n, p):\n    \"\"\"Make a binomial Pmf.\"\"\"\n    ks = np.arange(n+1)\n    ps = binom.pmf(ks, n, p)\n    return Pmf(ps, ks)\n\n\n\n\nCode\npmf_k = make_binomial(250, 0.2)\nplt.plot(pmf_k.qs, pmf_k.ps)\n\n\n\n\n\n\n\nCode\n# recover the highest probability\npmf_k.max_prob()\n\n\n50\n\n\nNow try to repeat using the updating mechanism, and the machinery of Bayes\nPrepare a uniform prior on the parameter that defines your ‘coin’ or the probability that any patient you pick in the hospital will be discharged tomorrow.\nThe key point below is that the prior is the probability assigned to each hypothesis (hypos in the code snippet).\n\n\nCode\nhypos = np.linspace(0, 1, 101) # hypos = hypotheses\nprior = Pmf(1, hypos)\nplt.plot(prior) # conveniently uses the index of the series for the 'x'\n\n\n\n\n\nNow use the hypotheses to return the likelihood. Remember the likelihood is the probability of the data given each hypothesis. Here we have 101 possible hypotheses, and we observe 1 discharge (initially). We are therefore asking what the chance of observing a discharge is given each hypothesis.\nPay attention because we are using the hypothesis (hypos) not the prior.\n\nStarting with the hypothesis that the parameter is 0 (i.e. the patient will not be discharged) then the likelihood of observing a discharge with that hypothesis must be 0. [Equivalent to a trick coin with two tails]\nNow hypothesising that the parameter is 1 (the patient will be discharged) then the likelihood of observing a discharge given that hypothesis must be 1. [Equivalent to a trick coin with two heads]\nAnd extending this then when the hypothesis is 0.01, 0.02, … then the likelihood of observing a discharge given that hypothesis must also be 0.01, 0.02, … [Equivalent to biased or weighted coins]\n\nThe likelihood defined below returns the probability of the data (i.e. the observation aka the observed events; in this case either a discharge or a non-discharge). But here we introduce a parameter that controls this probability. We are using a ‘coin’ and the notion of whether the coin is fair is controlled by that parameter.\nThe mental leap that we make here is from the ‘bowls’ (chocolate cookies) problem. - 2 bowls then the likelihood is mechanically 0.5 if the bowl contains 50% chocolate cookies; and 0.75 if it contains 75%. - 101 bowls then the likeliood is \\(0.00, 0.01, 0.02 ... 0.99, 1.00\\) depending on the bowl.\nHere we are imagining there are 101 different discharge statuses (or coin types), and we wish to know which discharge status (or coin type) we happen to be working with.\n\n\nCode\n\n# prob of discharge given the hypotheses\nlikelihood_discharge = hypos\n# prob of not being discharged given the hypotheses\nlikelihood_inpatient = 1 - hypos \n\n# wrapped into a dictionary\nlikelihood = {\n    'D': likelihood_discharge,\n    'I': likelihood_inpatient\n}\n\n\nNow we start to watch the discharges to see if there’s a problem with our assumption that the discharge rate is around 20%. We take our list of patients from yesterday and then start to check to see if they’ve gone home or not.\nPatient 1 is still an inpatient.\n\n\nCode\nposterior = prior * likelihood['I']\nplt.plot(posterior)\n\n\n\n\n\nThe updated probability distribution makes sense. It assigns zero chance to the discharge rate being 100% because we have now seen one failed discharge. The most likely hypothesis is that the discharge rate is 0% but it could be anything from 0 to 99%. So we don’t know much more.\nLet’s keep going.\nPatient 2 also did not get discharged.\n\n\nCode\n# the former posterior now becomes the prior\nprior = posterior.copy()\nposterior = prior * likelihood['I']\nplt.plot(posterior)\n\n\n\n\n\nContinuing our checks then patients 3, 4 and 5 also did not get discharged but patient 6 was discharged.\n\n\nCode\n# patient 3\n# using the previous posterior as the new prior\nposterior = posterior  * likelihood['I']\n# patient 4\nposterior = posterior  * likelihood['I']\n# patient 5\nposterior = posterior  * likelihood['I']\n# patient 6\nposterior = posterior  * likelihood['D'] # <- discharge!!\nplt.plot(posterior)\n\n\n\n\n\nLet’s wrap this up into a slightly more efficient piece of code\n\n\nCode\nfig = plt.figure(figsize=(10, 7))\nrows = 3\ncols = 3\nfig, ax = plt.subplots(rows, cols, sharex=True, sharey=True)\n\nprior = Pmf(1, hypos)\ndischarges = \"IIIIIDIII\" # 8 inpatients, 1 discharge\nposterior = prior.copy()\nfor row in range(rows):\n    for col in range(cols):\n        patient = discharges[(row * cols) + col]\n        print(row, col, patient)\n        posterior = posterior * likelihood[patient]\n        posterior.normalize() # scale by dividing by total unscaled\n        ax[row, col].plot(posterior)\nfig.tight_layout()\n\n\n0 0 I\n0 1 I\n0 2 I\n1 0 I\n1 1 I\n1 2 D\n2 0 I\n2 1 I\n2 2 I\n\n\n<Figure size 1000x700 with 0 Axes>\n\n\n\n\n\n2022-09-27 | work in progress | next steps\n\nwork with a coarser grid or build an example to make it clear that we’re sneaking up on the continuous distributions from a discrete\nrepeat the above using a the binomial rather than multiplying each time (diachronic)\nwork with different priors (triangle etc.)\n\nNotes from ThinkBayes\n\nThe other difference is the nature of what we are estimating. In the 101 Bowls Problem, we choose the bowl randomly, so it is uncontroversial to compute the probability of choosing each bowl. In the Euro Problem, the proportion of heads is a physical property of a given coin. Under some interpretations of probability, that’s a problem because physical properties are not considered random.\n\n\nDowney, Allen B.. Think Bayes (p. 61). O’Reilly Media. Kindle Edition."
  },
  {
    "objectID": "vignettes/hello-world.html",
    "href": "vignettes/hello-world.html",
    "title": "HyStakes",
    "section": "",
    "text": "Use this to confirm that your set-up is working. It should open and run from within VSCode, and it should be using the hystakes kernel.\n\nprint('Hello World!')\n\nHello World!"
  },
  {
    "objectID": "vignettes/prepare-synthetic-data.html",
    "href": "vignettes/prepare-synthetic-data.html",
    "title": "HyStakes",
    "section": "",
    "text": "Steve Harris\n2022-09-11\n\nWe need synthetic data.\nWe will inspect live data in this notebook, and then derive a short script that will generate suitable dummy data.\nNOTE\nThis notebook will be be run manually.\nThat should happen from the project root directory where the readme.md and the .env file are stored.\nThe following changes to the project root assuming that the notebook kernel is normally starting from the same directory as the notebook itself.\n\n%cd ../..\n\n/data/hymind/home/steve/HyStakes\n\n\n\nimport os\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import poisson, lognorm\nimport sqlalchemy as sa\n\n\n%matplotlib inline\n\n\n\nLoad environment variables and set-up SQLAlchemy connection engine for the EMAP Star\n\nload_dotenv(dotenv_path=\".env\")\ntry:\n    assert os.getenv(\"DOTENV_FILE_EXISTS\") == 'TRUE'\nexcept AssertionError:\n    print(\"!!! ERROR: check that the .env file exists at the top level of the project\")\n    print(\"!!! ERROR: check that the relative path is correct\")\n\n\n# Construct the PostgreSQL connection\nuds_host = os.getenv('EMAP_DB_HOST')\nuds_name = os.getenv('EMAP_DB_NAME')\nuds_port = os.getenv('EMAP_DB_PORT')\nuds_user = os.getenv('EMAP_DB_USER')\nuds_passwd = os.getenv('EMAP_DB_PASSWORD')\n\nemapdb_engine = sa.create_engine(f'postgresql://{uds_user}:{uds_passwd}@{uds_host}:{uds_port}/{uds_name}')\n\n\n\n\nHere’s the very long query that I built.\nIt selects all patients on tower wards 2 weeks ago (336 hours), and then finds the age (rounded) and the last heart rate. The remaining data has no identifiers.\n\nSELECT\n left(md5(lv.hospital_visit_id::TEXT), 6) id\n--,lv.location_visit_id\n--,lv.admission_datetime admit_dt_bed\n--,lv.discharge_datetime disch_dt_bed\n--,hv.admission_datetime admit_dt_hosp\n--,hv.discharge_datetime disch_dt_hosp\n,ROUND(EXTRACT(epoch FROM \n    (hv.discharge_datetime - (NOW() - '336 HOURS'::INTERVAL)\n    ))/3600) hours_to_discharge\n--,hv.discharge_destination\n--,hv.patient_class\n--,lo.location_string\n,dept.name department\n\n-- include age rounded to 5\n, ROUND(DATE_PART('year',AGE(cd.date_of_birth ))/5) * 5 AGE\n\n-- add last heart rate\n,hr.value_as_real pulse\n--,hr.observation_datetime\n\n\nFROM star.location_visit lv\nLEFT JOIN star.location lo ON lv.location_id = lo.location_id\nLEFT JOIN star.department dept ON lo.department_id = dept.department_id \nLEFT JOIN star.hospital_visit hv ON lv.hospital_visit_id = hv.hospital_visit_id\nLEFT JOIN star.core_demographic cd ON hv.mrn_id = cd.mrn_id\nLEFT JOIN (\n    WITH obs AS (\n        SELECT\n\n         vo.visit_observation_id\n        ,vo.hospital_visit_id\n        ,vo.observation_datetime\n        ,vo.value_as_real\n        ,ot.name\n\n        FROM star.visit_observation vo\n        LEFT JOIN star.visit_observation_type ot ON vo.visit_observation_type_id = ot.visit_observation_type_id\n        WHERE \n        ot.id_in_application = '8' -- heart rate\n        AND\n        vo.observation_datetime < NOW() - '336 HOURS'::INTERVAL \n        AND\n        vo.observation_datetime > NOW() - '360 HOURS'::INTERVAL \n    ),\n    obs_tail AS (\n        SELECT \n         obs.*\n        ,row_number() over (partition BY obs.hospital_visit_id ORDER BY obs.observation_datetime DESC) obs_tail\n        FROM obs\n    )\n    SELECT \n     visit_observation_id\n    ,hospital_visit_id\n    ,observation_datetime\n    ,value_as_real\n    ,NAME\n    FROM obs_tail \n    WHERE obs_tail = 1\n) hr -- heart rate\nON lv.hospital_visit_id = hr.hospital_visit_id\n\nWHERE \ndept.name IN (\n'UCH T03 INTENSIVE CARE'\n,'UCH SDEC'\n,'UCH T01 ACUTE MEDICAL'\n,'UCH T01 ENHANCED CARE'\n,'UCH T06 HEAD (T06H)'\n,'UCH T06 CENTRAL (T06C)'\n,'UCH T06 SOUTH PACU'\n,'UCH T06 GYNAE (T06G)'\n,'UCH T07 NORTH (T07N)'\n,'UCH T07 CV SURGE'\n,'UCH T07 SOUTH'\n,'UCH T07 SOUTH (T07S)'\n,'UCH T07 HDRU'\n,'UCH T08 NORTH (T08N)'\n,'UCH T08 SOUTH (T08S)'\n,'UCH T08S ARCU'\n,'UCH T09 SOUTH (T09S)'\n,'UCH T09 NORTH (T09N)'\n,'UCH T09 CENTRAL (T09C)'\n,'UCH T10 SOUTH (T10S)'\n,'UCH T10 NORTH (T10N)'\n,'UCH T10 MED (T10M)'\n,'UCH T11 SOUTH (T11S)'\n,'UCH T11 NORTH (T11N)'\n,'UCH T11 EAST (T11E)'\n,'UCH T11 NORTH (T11NO)'\n,'UCH T12 SOUTH (T12S)'\n,'UCH T12 NORTH (T12N)'\n,'UCH T13 SOUTH (T13S)'\n,'UCH T13 NORTH ONCOLOGY'\n,'UCH T13 NORTH (T13N)'\n,'UCH T14 NORTH TRAUMA'\n,'UCH T14 NORTH (T14N)'\n,'UCH T14 SOUTH ASU'\n,'UCH T14 SOUTH (T14S)'\n,'UCH T15 SOUTH DECANT'\n,'UCH T15 SOUTH (T15S)'\n,'UCH T15 NORTH (T15N)'\n,'UCH T15 NORTH DECANT'\n,'UCH T16 NORTH (T16N)'\n,'UCH T16 SOUTH (T16S)'\n,'UCH T16 SOUTH WINTER'\n\n)\nAND\nlv.admission_datetime < NOW() - '336 HOURS'::INTERVAL \nAND\n    (lv.discharge_datetime > NOW() - '336 HOURS'::INTERVAL \n     OR\n      (lv.discharge_datetime IS NULL AND hv.discharge_datetime IS NULL)\n    )\nAND \nlo.location_string NOT LIKE '%WAIT%'\nAND \nlo.location_string NOT LIKE '%null%'\n;\nTo benefit from syntax highlighting, proper version control and other goodness then we’ll actually load the query from its own file. The list above is there just to improve the flow and readability of this notebook.\n\nq = Path('utils/queries/discharges_dummy.sql').read_text()\n\n# this handles escaping etc (e.g. % in the LIKE clause becomes %%)\nq = sa.text(q)\n\n\ndf = pd.read_sql_query(q, emapdb_engine)\n\nNow inspect the fields\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 444 entries, 0 to 443\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   id                  444 non-null    object \n 1   hours_to_discharge  263 non-null    float64\n 2   department          444 non-null    object \n 3   age                 444 non-null    float64\n 4   pulse               381 non-null    float64\ndtypes: float64(3), object(2)\nmemory usage: 17.5+ KB\n\n\n\n\n\n\n\nPlot the distribution, then pick a distribution and create a simple sample\n\nLoS = df['hours_to_discharge']\nLoS.describe()\n\ncount    263.000000\nmean     108.049430\nstd       82.814341\nmin        1.000000\n25%       43.000000\n50%       86.000000\n75%      157.500000\nmax      332.000000\nName: hours_to_discharge, dtype: float64\n\n\n\nn, bins, patches = plt.hist(LoS, bins=20)\n\n\n\n\nNow generate an empirical distribution that roughly matches as above by sampling with replacement\n\nLoS_sample = LoS.sample(frac=1, replace=True)\nLoS_sample.describe()\n\ncount    263.000000\nmean      96.634981\nstd       80.698500\nmin        1.000000\n25%       40.000000\n50%       68.000000\n75%      138.500000\nmax      305.000000\nName: hours_to_discharge, dtype: float64\n\n\n\nn, bins, patches = plt.hist(LoS_sample, bins=20)\n\n\n\n\n\n\n\nPlot the distribution, then pick a distribution and create a simple sample\n\ndepartment = df['department']\ndepartment.value_counts()\n\nUCH T01 ACUTE MEDICAL     36\nUCH T10 SOUTH (T10S)      35\nUCH T10 NORTH (T10N)      24\nUCH T09 NORTH (T09N)      24\nUCH T13 NORTH ONCOLOGY    24\nUCH SDEC                  23\nUCH T13 SOUTH (T13S)      22\nUCH T08 SOUTH (T08S)      21\nUCH T08 NORTH (T08N)      20\nUCH T09 CENTRAL (T09C)    19\nUCH T03 INTENSIVE CARE    19\nUCH T12 NORTH (T12N)      18\nUCH T14 NORTH TRAUMA      17\nUCH T16 NORTH (T16N)      15\nUCH T07 NORTH (T07N)      15\nUCH T16 SOUTH WINTER      14\nUCH T14 SOUTH ASU         13\nUCH T06 HEAD (T06H)       12\nUCH T12 SOUTH (T12S)      10\nUCH T11 NORTH (T11N)       9\nUCH T01 ENHANCED CARE      9\nUCH T11 SOUTH (T11S)       9\nUCH T07 SOUTH              7\nUCH T06 CENTRAL (T06C)     6\nUCH T14 NORTH (T14N)       4\nUCH T06 SOUTH PACU         4\nUCH T09 SOUTH (T09S)       3\nUCH T06 GYNAE (T06G)       3\nUCH T15 SOUTH (T15S)       2\nUCH T07 SOUTH (T07S)       2\nUCH T10 MED (T10M)         1\nUCH T08S ARCU              1\nUCH T15 NORTH (T15N)       1\nUCH T11 EAST (T11E)        1\nUCH T16 SOUTH (T16S)       1\nName: department, dtype: int64\n\n\n\nX = df.groupby('department')['department'].count()\n\n\nfig, ax = plt.subplots(dpi=1.5*72)\nax.bar(X.index, X.values)\nax.tick_params(axis='x', rotation=90)\nplt.show()\n\n\n\n\nNow generate an empirical distribution that roughly matches as above by sampling with replacement\n\ndepartment_sample =department.sample(frac=1, replace=True)\ndepartment_sample.value_counts()\n\nUCH T01 ACUTE MEDICAL     45\nUCH T10 SOUTH (T10S)      37\nUCH T09 CENTRAL (T09C)    30\nUCH T13 NORTH ONCOLOGY    26\nUCH T10 NORTH (T10N)      26\nUCH T09 NORTH (T09N)      25\nUCH T13 SOUTH (T13S)      23\nUCH T03 INTENSIVE CARE    22\nUCH SDEC                  22\nUCH T07 NORTH (T07N)      21\nUCH T06 HEAD (T06H)       16\nUCH T12 NORTH (T12N)      16\nUCH T16 NORTH (T16N)      16\nUCH T14 SOUTH ASU         14\nUCH T08 SOUTH (T08S)      14\nUCH T14 NORTH TRAUMA      12\nUCH T16 SOUTH WINTER      12\nUCH T08 NORTH (T08N)      11\nUCH T11 NORTH (T11N)      10\nUCH T11 SOUTH (T11S)       7\nUCH T01 ENHANCED CARE      6\nUCH T07 SOUTH              6\nUCH T12 SOUTH (T12S)       5\nUCH T06 SOUTH PACU         5\nUCH T14 NORTH (T14N)       3\nUCH T06 CENTRAL (T06C)     3\nUCH T06 GYNAE (T06G)       3\nUCH T15 SOUTH (T15S)       3\nUCH T07 SOUTH (T07S)       1\nUCH T15 NORTH (T15N)       1\nUCH T16 SOUTH (T16S)       1\nUCH T10 MED (T10M)         1\nUCH T09 SOUTH (T09S)       1\nName: department, dtype: int64\n\n\n\nX_s = department_sample.value_counts()\nX_s = X_s.reindex(X.index)\nfig, ax = plt.subplots(dpi=1.5*72)\nax.bar(X_s.index, X_s.values)\nax.tick_params(axis='x', rotation=90)\nplt.show()\n\n\n\n\n\n\n\nPlot the distribution, then pick a distribution and create a simple sample\n\nage = df['age']\nage.describe()\n\ncount    444.000000\nmean      57.927928\nstd       23.748289\nmin        0.000000\n25%       40.000000\n50%       65.000000\n75%       75.000000\nmax      100.000000\nName: age, dtype: float64\n\n\n\nn, bins, patches = plt.hist(age, bins=20)\n\n\n\n\nNow generate an empirical distribution that roughly matches as above by sampling with replacement\n\nage_sample = age.sample(frac=1, replace=True)\nage_sample.describe()\n\ncount    444.000000\nmean      58.355856\nstd       22.933793\nmin        5.000000\n25%       43.750000\n50%       65.000000\n75%       75.000000\nmax      100.000000\nName: age, dtype: float64\n\n\n\nn, bins, patches = plt.hist(age_sample, bins=20)\n\n\n\n\n\n\n\nPlot the distribution, then pick a distribution and create a simple sample\n\npulse = df['pulse']\npulse.describe()\n\ncount    381.000000\nmean      81.391076\nstd       17.147712\nmin       42.000000\n25%       70.000000\n50%       80.000000\n75%       90.000000\nmax      187.000000\nName: pulse, dtype: float64\n\n\n\nn, bins, patches = plt.hist(pulse, bins=20)\n\n\n\n\nNow generate an empirical distribution that roughly matches as above by sampling with replacement\n\npulse_sample = pulse.sample(frac=1, replace=True)\npulse_sample.describe()\n\ncount    375.000000\nmean      79.696000\nstd       15.435336\nmin       46.000000\n25%       68.500000\n50%       78.000000\n75%       89.000000\nmax      133.000000\nName: pulse, dtype: float64\n\n\n\nn, bins, patches = plt.hist(pulse_sample, bins=20)\n\n\n\n\n\npulse_sample\n\n115     59.0\n363    102.0\n317     72.0\n257     60.0\n265      NaN\n       ...  \n237      NaN\n118     87.0\n47      78.0\n253     63.0\n394     77.0\nName: pulse, Length: 444, dtype: float64\n\n\n\n\n\n\n\nLoS_sample.reset_index(inplace=True, drop=True)\ndepartment_sample.reset_index(inplace=True, drop=True)\nage_sample.reset_index(inplace=True, drop=True)\npulse_sample.reset_index(inplace=True, drop=True)\n\n\ndf_dummy = pd.concat([LoS_sample, department_sample, age_sample, pulse_sample], axis=1)\n\n\ndf_dummy.reset_index(inplace=True)\n\n\ndf_dummy.rename({'index': 'id'}, axis=1, inplace=True)\n\n\ndf_dummy.head()\n\n\n\n\n\n  \n    \n      \n      id\n      hours_to_discharge\n      department\n      age\n      pulse\n    \n  \n  \n    \n      0\n      0\n      42.0\n      UCH T11 NORTH (T11N)\n      60.0\n      59.0\n    \n    \n      1\n      1\n      NaN\n      UCH T03 INTENSIVE CARE\n      65.0\n      102.0\n    \n    \n      2\n      2\n      132.0\n      UCH T13 NORTH ONCOLOGY\n      60.0\n      72.0\n    \n    \n      3\n      3\n      206.0\n      UCH T01 ACUTE MEDICAL\n      10.0\n      60.0\n    \n    \n      4\n      4\n      90.0\n      UCH T11 SOUTH (T11S)\n      65.0\n      NaN\n    \n  \n\n\n\n\nWrite dummy data out. Use sqlite to manage typing etc.\n\nsqlite_engine = sa.create_engine('sqlite:///data/dummy.db')\ndf_dummy.to_sql('discharges', con=sqlite_engine, if_exists='replace') \n\nNow demonstrate that you can use those data\n\n# remember `///` for sqlite file paths\nengine = sa.create_engine('sqlite:///../../data/dummy.db')\ndf = pd.read_sql('select * from discharges', engine)"
  },
  {
    "objectID": "vignettes/pymc-setup.html",
    "href": "vignettes/pymc-setup.html",
    "title": "HyStakes",
    "section": "",
    "text": "Walk through as per the installation instructions https://www.pymc.io/projects/docs/en/latest/learn/core_notebooks/pymc_overview.html\n\nimport arviz as az\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sqlalchemy as sa\n\n\n%config InlineBackend.figure_format = 'retina'\n# Initialize random number generator\nRANDOM_SEED = 8927\nrng = np.random.default_rng(RANDOM_SEED)\naz.style.use(\"arviz-darkgrid\")\n\n\n# True parameter values\nalpha, sigma = 1, 1\nbeta = [1, 2.5]\n\n# Size of dataset\nsize = 100\n\n# Predictor variable\nX1 = np.random.randn(size)\nX2 = np.random.randn(size) * 0.2\n\n# Simulate outcome variable\nY = alpha + beta[0] * X1 + beta[1] * X2 + rng.normal(size=size) * sigma\n\n\nfig, axes = plt.subplots(1, 2, sharex=True, figsize=(10, 4))\naxes[0].scatter(X1, Y, alpha=0.6)\naxes[1].scatter(X2, Y, alpha=0.6)\naxes[0].set_ylabel(\"Y\")\naxes[0].set_xlabel(\"X1\")\naxes[1].set_xlabel(\"X2\");\n\n\n\n\n\nimport pymc as pm\n\nprint(f\"Running on PyMC v{pm.__version__}\")\n\nRunning on PyMC v4.2.2\n\n\n\nbasic_model = pm.Model()\n\nwith basic_model:\n\n    # Priors for unknown model parameters\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n    beta = pm.Normal(\"beta\", mu=0, sigma=10, shape=2)\n    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n\n    # Expected value of outcome\n    mu = alpha + beta[0] * X1 + beta[1] * X2\n\n    # Likelihood (sampling distribution) of observations\n    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=Y)\n\n\nwith basic_model:\n    # draw 1000 posterior samples\n    idata = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, beta, sigma]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 7 seconds.\n\n\n\nidata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (chain: 4, draw: 1000, beta_dim_0: 2)\nCoordinates:\n  * chain       (chain) int64 0 1 2 3\n  * draw        (draw) int64 0 1 2 3 4 5 6 7 ... 992 993 994 995 996 997 998 999\n  * beta_dim_0  (beta_dim_0) int64 0 1\nData variables:\n    alpha       (chain, draw) float64 1.222 1.061 1.2 ... 1.122 1.139 1.144\n    beta        (chain, draw, beta_dim_0) float64 1.126 3.113 ... 0.9826 3.033\n    sigma       (chain, draw) float64 0.9843 1.059 1.01 ... 0.945 0.9402 0.9509\nAttributes:\n    created_at:                 2022-10-18T21:29:30.973443\n    arviz_version:              0.12.1\n    inference_library:          pymc\n    inference_library_version:  4.2.2\n    sampling_time:              7.077733039855957\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 1000beta_dim_0: 2Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])beta_dim_0(beta_dim_0)int640 1array([0, 1])Data variables: (3)alpha(chain, draw)float641.222 1.061 1.2 ... 1.139 1.144array([[1.22182606, 1.06122449, 1.19960544, ..., 1.05709566, 1.20642578,\n        1.22892916],\n       [1.33032996, 1.06347311, 1.07428442, ..., 1.22247521, 1.26607136,\n        1.24936237],\n       [0.99688654, 1.25084385, 1.05605624, ..., 1.07920476, 1.17454768,\n        1.08956216],\n       [1.15648514, 1.09849708, 1.05702968, ..., 1.12169869, 1.13935497,\n        1.14357447]])beta(chain, draw, beta_dim_0)float641.126 3.113 0.7162 ... 0.9826 3.033array([[[1.1264105 , 3.11273215],\n        [0.7162184 , 1.79977504],\n        [1.1085078 , 3.29487963],\n        ...,\n        [0.7711776 , 2.22455605],\n        [1.0579948 , 3.52906373],\n        [0.8900915 , 2.02386965]],\n\n       [[0.9491352 , 2.65501654],\n        [0.88381715, 2.83347382],\n        [0.96552764, 2.65922829],\n        ...,\n        [0.86363921, 2.77089917],\n        [0.73685976, 3.26925474],\n        [0.96882616, 3.53468554]],\n\n       [[1.01384624, 2.804141  ],\n        [0.85603965, 2.43824496],\n        [0.99318084, 2.68287852],\n        ...,\n        [1.03507824, 3.16936456],\n        [1.18872934, 3.32001642],\n        [0.922849  , 2.79709889]],\n\n       [[0.90702263, 2.63937881],\n        [0.8176043 , 2.56107499],\n        [0.95720254, 2.83066387],\n        ...,\n        [0.9859796 , 2.09272291],\n        [0.98475309, 2.23267624],\n        [0.98259939, 3.03316791]]])sigma(chain, draw)float640.9843 1.059 1.01 ... 0.9402 0.9509array([[0.98431474, 1.05918995, 1.00983798, ..., 1.07613078, 0.96987762,\n        0.97456049],\n       [1.10547799, 0.91442614, 0.95355338, ..., 1.04604503, 1.08698869,\n        1.043467  ],\n       [0.98514381, 1.04647217, 0.96453107, ..., 1.01742128, 1.05502611,\n        1.04897851],\n       [1.07772792, 0.99235417, 0.93730655, ..., 0.94502123, 0.94023425,\n        0.95087513]])Attributes: (6)created_at :2022-10-18T21:29:30.973443arviz_version :0.12.1inference_library :pymcinference_library_version :4.2.2sampling_time :7.077733039855957tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (chain: 4, draw: 1000, Y_obs_dim_0: 100)\nCoordinates:\n  * chain        (chain) int64 0 1 2 3\n  * draw         (draw) int64 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\n  * Y_obs_dim_0  (Y_obs_dim_0) int64 0 1 2 3 4 5 6 7 ... 92 93 94 95 96 97 98 99\nData variables:\n    Y_obs        (chain, draw, Y_obs_dim_0) float64 -1.033 -1.48 ... -0.9122\nAttributes:\n    created_at:                 2022-10-18T21:29:31.095144\n    arviz_version:              0.12.1\n    inference_library:          pymc\n    inference_library_version:  4.2.2xarray.DatasetDimensions:chain: 4draw: 1000Y_obs_dim_0: 100Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Y_obs_dim_0(Y_obs_dim_0)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])Data variables: (1)Y_obs(chain, draw, Y_obs_dim_0)float64-1.033 -1.48 ... -0.8688 -0.9122array([[[-1.03289891, -1.47955468, -1.36587523, ..., -1.36301098,\n         -0.90375287, -0.93962964],\n        [-0.9822567 , -1.71046122, -1.04005208, ..., -2.26804723,\n         -0.98433256, -1.02332328],\n        [-1.03003636, -1.47061227, -1.36138397, ..., -1.38795249,\n         -0.92876961, -0.96351075],\n        ...,\n        [-0.99906534, -1.66343715, -1.08453195, ..., -2.09887682,\n         -0.99651215, -1.03766473],\n        [-0.97727655, -1.42186585, -1.3676575 , ..., -1.41356688,\n         -0.89174697, -0.91366792],\n        [-0.97677998, -1.56575517, -1.13201064, ..., -1.80396882,\n         -0.89472714, -0.91959883]],\n\n       [[-1.1236545 , -1.39846107, -1.34603969, ..., -1.49873537,\n         -1.02523976, -1.02375392],\n        [-0.84850088, -1.67691927, -1.05522326, ..., -2.00978924,\n         -0.83182283, -0.89372878],\n        [-0.90890385, -1.6930276 , -1.10192217, ..., -1.82908035,\n         -0.88314713, -0.94622928],\n...\n        [-0.9759459 , -1.596792  , -1.2157159 , ..., -1.61331232,\n         -0.94082758, -0.99898566],\n        [-1.07953718, -1.50928019, -1.38889664, ..., -1.33608638,\n         -0.97614629, -1.02088781],\n        [-0.99205748, -1.59815152, -1.16319833, ..., -1.78085277,\n         -0.96916517, -1.01419908]],\n\n       [[-1.032734  , -1.54164008, -1.20121145, ..., -1.72759567,\n         -0.99403465, -1.02289874],\n        [-0.92781028, -1.61085226, -1.07930913, ..., -2.01232892,\n         -0.91168742, -0.95125469],\n        [-0.88355605, -1.69515527, -1.09377927, ..., -1.85909844,\n         -0.86308148, -0.93200498],\n        ...,\n        [-0.9345386 , -1.74089237, -1.0842748 , ..., -1.81909287,\n         -0.88974398, -0.94005487],\n        [-0.93351951, -1.69465065, -1.10779739, ..., -1.7790554 ,\n         -0.87484952, -0.92426252],\n        [-0.92527429, -1.54921383, -1.20252093, ..., -1.65539239,\n         -0.86877513, -0.91215463]]])Attributes: (4)created_at :2022-10-18T21:29:31.095144arviz_version :0.12.1inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:              (chain: 4, draw: 1000)\nCoordinates:\n  * chain                (chain) int64 0 1 2 3\n  * draw                 (draw) int64 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables: (12/16)\n    process_time_diff    (chain, draw) float64 0.000211 0.000206 ... 0.000199\n    step_size_bar        (chain, draw) float64 0.9423 0.9423 ... 0.8989 0.8989\n    perf_counter_start   (chain, draw) float64 8.278e+05 8.278e+05 ... 8.278e+05\n    smallest_eigval      (chain, draw) float64 nan nan nan nan ... nan nan nan\n    acceptance_rate      (chain, draw) float64 1.0 0.8348 0.9676 ... 1.0 0.6763\n    step_size            (chain, draw) float64 1.028 1.028 ... 0.7739 0.7739\n    ...                   ...\n    max_energy_error     (chain, draw) float64 -0.5733 0.3052 ... 0.9494\n    index_in_trajectory  (chain, draw) int64 -3 3 3 -3 -3 1 ... -3 1 2 -1 1 -2\n    lp                   (chain, draw) float64 -153.8 -155.0 ... -151.7 -151.7\n    energy_error         (chain, draw) float64 -0.1409 0.3052 ... -0.03142\n    diverging            (chain, draw) bool False False False ... False False\n    n_steps              (chain, draw) float64 3.0 3.0 3.0 3.0 ... 3.0 1.0 3.0\nAttributes:\n    created_at:                 2022-10-18T21:29:30.977880\n    arviz_version:              0.12.1\n    inference_library:          pymc\n    inference_library_version:  4.2.2\n    sampling_time:              7.077733039855957\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (16)process_time_diff(chain, draw)float640.000211 0.000206 ... 0.000199array([[0.000211, 0.000206, 0.000209, ..., 0.000206, 0.000207, 0.000204],\n       [0.00021 , 0.000206, 0.000208, ..., 0.000206, 0.000197, 0.000206],\n       [0.000228, 0.000212, 0.00031 , ..., 0.000215, 0.000208, 0.000208],\n       [0.000262, 0.000207, 0.000423, ..., 0.0002  , 0.000101, 0.000199]])step_size_bar(chain, draw)float640.9423 0.9423 ... 0.8989 0.8989array([[0.94230254, 0.94230254, 0.94230254, ..., 0.94230254, 0.94230254,\n        0.94230254],\n       [1.01637376, 1.01637376, 1.01637376, ..., 1.01637376, 1.01637376,\n        1.01637376],\n       [0.9737365 , 0.9737365 , 0.9737365 , ..., 0.9737365 , 0.9737365 ,\n        0.9737365 ],\n       [0.89890096, 0.89890096, 0.89890096, ..., 0.89890096, 0.89890096,\n        0.89890096]])perf_counter_start(chain, draw)float648.278e+05 8.278e+05 ... 8.278e+05array([[827821.00044029, 827821.00069775, 827821.00095454, ...,\n        827821.28438337, 827821.28463304, 827821.28488829],\n       [827821.01577567, 827821.01603204, 827821.01628483, ...,\n        827821.31302804, 827821.31327908, 827821.31352179],\n       [827821.04958337, 827821.04986108, 827821.05036933, ...,\n        827821.37852775, 827821.37878792, 827821.37904367],\n       [827821.17636533, 827821.1766945 , 827821.17725183, ...,\n        827821.50319121, 827821.50343579, 827821.5035865 ]])smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])acceptance_rate(chain, draw)float641.0 0.8348 0.9676 ... 1.0 0.6763array([[1.        , 0.83484095, 0.96763695, ..., 0.96231975, 0.76928895,\n        1.        ],\n       [0.92980487, 1.        , 1.        , ..., 1.        , 0.71119349,\n        0.97137219],\n       [1.        , 0.86426917, 0.92196173, ..., 0.8808114 , 0.63612693,\n        1.        ],\n       [1.        , 0.9845486 , 0.85537648, ..., 0.97710576, 1.        ,\n        0.67633639]])step_size(chain, draw)float641.028 1.028 1.028 ... 0.7739 0.7739array([[1.02819307, 1.02819307, 1.02819307, ..., 1.02819307, 1.02819307,\n        1.02819307],\n       [1.02396486, 1.02396486, 1.02396486, ..., 1.02396486, 1.02396486,\n        1.02396486],\n       [0.96573466, 0.96573466, 0.96573466, ..., 0.96573466, 0.96573466,\n        0.96573466],\n       [0.77392676, 0.77392676, 0.77392676, ..., 0.77392676, 0.77392676,\n        0.77392676]])tree_depth(chain, draw)int642 2 2 2 2 2 2 2 ... 2 2 2 2 3 2 1 2array([[2, 2, 2, ..., 2, 2, 2],\n       [2, 2, 2, ..., 2, 2, 2],\n       [2, 2, 2, ..., 2, 2, 2],\n       [2, 2, 3, ..., 2, 1, 2]])perf_counter_diff(chain, draw)float640.0002111 0.0002061 ... 0.0001993array([[0.00021108, 0.00020612, 0.00020937, ..., 0.00020546, 0.0002075 ,\n        0.00020275],\n       [0.00020975, 0.00020596, 0.00020833, ..., 0.00020658, 0.00019746,\n        0.00020704],\n       [0.00022813, 0.00021296, 0.00031858, ..., 0.00021542, 0.00020667,\n        0.00020733],\n       [0.00027258, 0.000207  , 0.00042254, ..., 0.00020046, 0.00010054,\n        0.00019929]])largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])energy(chain, draw)float64155.1 156.2 156.0 ... 152.1 154.0array([[155.08290585, 156.17489626, 156.03641552, ..., 153.99356212,\n        156.85022181, 154.25862456],\n       [156.53205568, 154.02816693, 152.45454188, ..., 153.37348135,\n        154.4871091 , 155.53064548],\n       [154.73956858, 154.64130835, 153.44535365, ..., 159.66341873,\n        156.41736163, 156.08682365],\n       [154.68069048, 152.36809492, 153.15247872, ..., 154.2122196 ,\n        152.10963448, 153.95734991]])max_energy_error(chain, draw)float64-0.5733 0.3052 ... -0.06365 0.9494array([[-0.57330335,  0.30524228, -0.52655956, ..., -0.13364125,\n         0.55143278, -0.38848016],\n       [ 0.20813822, -0.25260752, -0.27797104, ..., -0.41351227,\n         0.70842967, -0.48651441],\n       [-0.37855763,  0.52288553,  0.173086  , ..., -1.82209597,\n         1.00537802, -0.96585162],\n       [-0.44531896, -0.11361702,  0.32468591, ..., -0.20009725,\n        -0.06365195,  0.94940895]])index_in_trajectory(chain, draw)int64-3 3 3 -3 -3 1 ... -3 1 2 -1 1 -2array([[-3,  3,  3, ...,  1,  3,  2],\n       [-2,  3, -1, ...,  2, -1,  2],\n       [ 2, -3,  3, ..., -3, -1,  1],\n       [ 2, -2,  1, ..., -1,  1, -2]])lp(chain, draw)float64-153.8 -155.0 ... -151.7 -151.7array([[-153.8130327 , -155.01227035, -153.7575581 , ..., -153.35120037,\n        -153.85200467, -152.29401523],\n       [-153.36067275, -152.36581837, -151.61310837, ..., -151.62737751,\n        -154.15795512, -153.02530327],\n       [-153.013511  , -152.04218213, -151.92416583, ..., -152.83197377,\n        -155.71481815, -151.63677866],\n       [-151.71287573, -151.7040821 , -152.10605209, ..., -152.0034649 ,\n        -151.74843851, -151.74303739]])energy_error(chain, draw)float64-0.1409 0.3052 ... -0.03142array([[-0.14085221,  0.30524228, -0.4172998 , ...,  0.11266457,\n         0.03685473, -0.32917505],\n       [-0.17803277, -0.25260752, -0.23716031, ..., -0.41351227,\n         0.44447061, -0.20402037],\n       [-0.21813926, -0.17459137, -0.06073929, ..., -1.82209597,\n         0.61161419, -0.96585162],\n       [-0.44531896, -0.01619643,  0.15021985, ..., -0.20009725,\n        -0.06365195, -0.03141722]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])n_steps(chain, draw)float643.0 3.0 3.0 3.0 ... 7.0 3.0 1.0 3.0array([[3., 3., 3., ..., 3., 3., 3.],\n       [3., 3., 3., ..., 3., 3., 3.],\n       [3., 3., 3., ..., 3., 3., 3.],\n       [3., 3., 7., ..., 3., 1., 3.]])Attributes: (6)created_at :2022-10-18T21:29:30.977880arviz_version :0.12.1inference_library :pymcinference_library_version :4.2.2sampling_time :7.077733039855957tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (Y_obs_dim_0: 100)\nCoordinates:\n  * Y_obs_dim_0  (Y_obs_dim_0) int64 0 1 2 3 4 5 6 7 ... 92 93 94 95 96 97 98 99\nData variables:\n    Y_obs        (Y_obs_dim_0) float64 1.373 2.396 1.33 ... 3.757 0.9926 1.171\nAttributes:\n    created_at:                 2022-10-18T21:29:31.095939\n    arviz_version:              0.12.1\n    inference_library:          pymc\n    inference_library_version:  4.2.2xarray.DatasetDimensions:Y_obs_dim_0: 100Coordinates: (1)Y_obs_dim_0(Y_obs_dim_0)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])Data variables: (1)Y_obs(Y_obs_dim_0)float641.373 2.396 1.33 ... 0.9926 1.171array([ 1.37317951,  2.39645213,  1.32992816,  2.67198088,  2.47738595,\n        2.14712209,  1.71085036, -0.5604959 , -1.39084635,  0.11709389,\n       -0.50481176, -0.07831487,  2.87770011,  0.37534412,  0.4772861 ,\n        1.60552596,  0.29814138,  1.48120947,  0.22697285,  2.98648645,\n        0.12087456, -0.85020213,  1.52687659,  0.52346378, -0.51146109,\n        0.65139347, -1.36619736,  0.2247007 ,  0.91037543,  1.09180962,\n        0.27725773,  4.33815747,  1.52626104,  1.60220318, -1.17307117,\n       -0.65757661,  1.32492076,  0.22681271,  2.33499488, -2.33269465,\n        0.84353946,  0.14096057,  1.15685232,  1.75990594,  1.25471341,\n       -0.26309215, -1.51751987,  0.91931753,  2.07005011,  2.52750088,\n        1.33720271, -0.41851447,  1.01675173,  3.99685181,  0.77144083,\n        1.96043615,  2.66175521,  2.18539861,  0.36408549, -2.08148573,\n        1.40074149,  1.18950234,  1.05260031,  1.00861042,  2.71997535,\n        2.40357859,  3.87519893,  1.30603085,  1.59892429, -0.09543246,\n        0.80718855, -0.65565292,  0.86288868,  0.56283481,  1.95189123,\n        2.34793097,  2.76971285,  1.79700517,  2.96483704,  0.0265968 ,\n        3.50306213,  2.54803236, -1.77433472, -1.07771345,  3.95663401,\n        1.74047638,  0.14587767, -0.095623  ,  2.73008959,  1.53969348,\n        0.14060668, -2.20505403,  0.468906  ,  3.72788792, -0.04096889,\n       -2.0411723 ,  1.75863409,  3.75686128,  0.99263902,  1.17133638])Attributes: (4)created_at :2022-10-18T21:29:31.095939arviz_version :0.12.1inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\nidata.posterior[\"alpha\"].sel(draw=slice(0, 4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'alpha' (chain: 4, draw: 5)>\narray([[1.22182606, 1.06122449, 1.19960544, 1.11622334, 1.17347533],\n       [1.33032996, 1.06347311, 1.07428442, 1.12094194, 1.28404611],\n       [0.99688654, 1.25084385, 1.05605624, 1.28236762, 1.28115075],\n       [1.15648514, 1.09849708, 1.05702968, 1.23564325, 1.16488626]])\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4xarray.DataArray'alpha'chain: 4draw: 51.222 1.061 1.2 1.116 1.173 1.33 ... 1.156 1.098 1.057 1.236 1.165array([[1.22182606, 1.06122449, 1.19960544, 1.11622334, 1.17347533],\n       [1.33032996, 1.06347311, 1.07428442, 1.12094194, 1.28404611],\n       [0.99688654, 1.25084385, 1.05605624, 1.28236762, 1.28115075],\n       [1.15648514, 1.09849708, 1.05702968, 1.23564325, 1.16488626]])Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4array([0, 1, 2, 3, 4])Attributes: (0)\n\n\n\nwith basic_model:\n    # instantiate sampler\n    step = pm.Slice()\n\n    # draw 5000 posterior samples\n    slice_idata = pm.sample(5000, step=step)\n\nMultiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n>Slice: [alpha]\n>Slice: [beta]\n>Slice: [sigma]\n\n\n\n\n\n\n\n    \n      \n      100.00% [24000/24000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 9 seconds.\n\n\n\naz.plot_trace(idata, combined=True);\n\n\n\n\n\naz.summary(idata, round_to=2)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      alpha\n      1.16\n      0.10\n      0.98\n      1.36\n      0.00\n      0.0\n      5564.31\n      3063.75\n      1.0\n    \n    \n      beta[0]\n      0.93\n      0.10\n      0.73\n      1.12\n      0.00\n      0.0\n      5600.72\n      3264.84\n      1.0\n    \n    \n      beta[1]\n      2.64\n      0.51\n      1.76\n      3.65\n      0.01\n      0.0\n      5761.89\n      3160.86\n      1.0\n    \n    \n      sigma\n      1.01\n      0.07\n      0.87\n      1.14\n      0.00\n      0.0\n      5343.73\n      3051.86\n      1.0"
  },
  {
    "objectID": "vignettes/think-bayes-ch8.html",
    "href": "vignettes/think-bayes-ch8.html",
    "title": "HyStakes",
    "section": "",
    "text": "The world cup problem\n\neach team has \\(\\lambda\\) a goal-scoring rate (goals per game)\ngoals are possible at any time in the game so the per minute probability of a goal is \\(\\lambda/90\\)\n\nUsing the Poisson distribution then the probability of scoring \\(k\\) goals is\n\\[\ne^{-\\lambda}\\frac{\\lambda^k}{k!}\n\\]\n\nfrom scipy.stats import poisson\nlam = 1.4\ndist = poisson(lam)\ntype(dist)\n\nscipy.stats._distn_infrastructure.rv_discrete_frozen\n\n\nThen use the ‘frozen’ random variable above to estimate the probability of a specific value of \\(k\\). In this context, this is equivalent to estimating the probability of 4 goals from a team that scores on average 1.4 goals per game.\n\nk = 4\ndist.pmf(4)\n\n0.039471954028253146\n\n\n\nfrom empiricaldist import Pmf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef make_poisson_pmf(lam, qs):\n    \"\"\"\n    Generate a probability mass function for a Poisson\n    with mean 'lam' over a range (array) of values 'qs'\n    \"\"\"\n    ps = poisson(lam).pmf(qs)\n    pmf = Pmf(ps, qs)\n    pmf.normalize()\n    return pmf\n\nlam = 1.4\ngoals = np.arange(10)\npmf_goals = make_poisson_pmf(lam, goals)\npmf_goals.bar()\n\n\n\n\nNow being a Bayesian, then we’re really not that interested in the forward problem. We actually want to estimate the goal scoring rate given a number of observed goals: the inverse problem.\nSo we need to conjecture a set of hypotheses that represents the possible values of \\(\\lambda\\) and then we need to prior that assigns a (prior) probability to each hypothesis.\nHere we choose to use the gamma distribution to model the continuous range of possible hypotheses (i.e. a continuous non-negative number). The shape of the gamma is defined by a single parameter \\(\\alpha\\) that represents its mean. And it so happens that the gamma is the conjugate prior of the Poisson.\n\nfrom scipy.stats import gamma\n\n# mean of gamma is our best guess estimate of goals per game \n# given that's the average from previous world cup matches\nalpha = 1.4 \nqs = np.linspace(0,10,101) # hypothesis grid\nps = gamma(alpha).pdf(qs)\n\nprior = Pmf(ps, qs)\nprior.normalize()\n\nprior.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nGiven an observed number of goals (4) then how does this change our the prior probability distribution over the range of hypotheses.\nHere we need to calculate the probabilty of the data (4 goals) for each hypothesis (i.e. the likelihood) which we then multiply by the prior to find the posterior.\n\n# worked example by hand after seeing a team score 4 goals\nhypos = np.linspace(0, 10, 101)\nlikelihood_unnorm = poisson([hypos]).pmf(4)\nlikelihood = likelihood_unnorm / np.sum(likelihood_unnorm)\nplt.plot(likelihood[0]) # <- used [0] b/c array is nested via poisson([])\n\n\n\n\n\ndef update_poisson(pmf, data):\n    \"\"\"Update Pmf with a Poisson likelihood\"\"\"\n    k = data\n    lams = pmf.qs # hypotheses over a grid\n    likelihood = poisson(lams).pmf(k)\n    pmf *= likelihood\n    # pmf.normalize() # <- normalize returns a scaler??\n    # do this by hand your self\n    return pmf / pmf.sum()\n\n# prior\nalpha = 1.4 \nqs = np.linspace(0,10,101) # hypothesis grid\nps = gamma(alpha).pdf(qs)\nps = ps / ps.sum() # normalize\n\n\nprior = Pmf(ps, qs)\n\n# update (after seeing France score 4 goals)\nfrance = prior.copy()\nposterior_f = update_poisson(france, 4)   \n\nposterior_f.plot()\nplt.plot(prior.qs, prior.ps)\nplt.legend([\"Posterior\", \"Prior\"])\n\n<matplotlib.legend.Legend at 0x1483ee1f0>\n\n\n\n\n\nNow repeat the above for Croatia who have scored 2 goals (as opposed to the 4 goals scored by France above)\n\n# update (after seeing Croatia score 2 goals)\ncroatia = prior.copy()\nposterior_c = update_poisson(croatia, 2)   \n\nposterior_c.plot()\nplt.plot(prior.qs, prior.ps)\nplt.legend([\"Posterior\", \"Prior\"])\n\n<matplotlib.legend.Legend at 0x148560640>\n\n\n\n\n\n\nNow try to predict the results of a rematch\nWe don’t know the true goal scoring rate for each team (but we do have a posterior probability distribution).\nWe do know how to generate a goal scoring distribution for an individual value of lamda\nso we must first sample from the posterior repeatedly\nand then generate a distribution of goals from that sample\n\n\n# first sample from the posterior\nsamples = posterior_f.sample(10**5)\nplt.hist(samples, density=True, bins=np.arange(0,10,0.1))\n\n(array([0.00000000e+00, 0.00000000e+00, 1.00002000e-04, 1.40002800e-03,\n        7.40014800e-03, 1.80003600e-02, 3.04006080e-02, 4.68009360e-02,\n        7.37014740e-02, 9.94019880e-02, 1.28202564e-01, 1.56103122e-01,\n        1.92403848e-01, 2.29804596e-01, 2.55605112e-01, 2.78405568e-01,\n        3.01806036e-01, 3.23906478e-01, 3.25206504e-01, 3.53307066e-01,\n        3.75207504e-01, 3.64707294e-01, 3.66507330e-01, 3.66107322e-01,\n        3.75807516e-01, 3.68907378e-01, 3.51807036e-01, 3.36806736e-01,\n        3.25006500e-01, 3.12506250e-01, 2.92105842e-01, 2.75705514e-01,\n        2.53905078e-01, 2.54505090e-01, 2.31604632e-01, 2.14604292e-01,\n        2.02604052e-01, 1.86603732e-01, 1.68603372e-01, 1.53903078e-01,\n        1.40702814e-01, 1.27302546e-01, 1.18602372e-01, 1.11202224e-01,\n        1.04102082e-01, 9.05018100e-02, 7.36014720e-02, 7.37014740e-02,\n        6.56013120e-02, 5.41010820e-02, 4.74009480e-02, 4.64009280e-02,\n        3.90007800e-02, 3.57007140e-02, 3.47006940e-02, 2.58005160e-02,\n        2.55005100e-02, 2.15004300e-02, 2.00004000e-02, 1.73003460e-02,\n        1.57003140e-02, 1.48002960e-02, 1.21002420e-02, 1.16002320e-02,\n        8.30016600e-03, 6.90013800e-03, 7.40014800e-03, 6.90013800e-03,\n        6.30012600e-03, 5.20010400e-03, 4.90009800e-03, 4.60009200e-03,\n        3.70007400e-03, 3.60007200e-03, 2.10004200e-03, 1.90003800e-03,\n        1.60003200e-03, 1.00002000e-03, 9.00018000e-04, 1.20002400e-03,\n        9.00018000e-04, 8.00016000e-04, 5.00010000e-04, 1.00002000e-03,\n        7.00014000e-04, 4.00008000e-04, 4.00008000e-04, 5.00010000e-04,\n        3.00006000e-04, 5.00010000e-04, 2.00004000e-04, 1.00002000e-04,\n        0.00000000e+00, 1.00002000e-04, 1.00002000e-04, 0.00000000e+00,\n        1.00002000e-04, 1.00002000e-04, 4.00008000e-04]),\n array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2,\n        1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5,\n        2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8,\n        3.9, 4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5. , 5.1,\n        5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6. , 6.1, 6.2, 6.3, 6.4,\n        6.5, 6.6, 6.7, 6.8, 6.9, 7. , 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7,\n        7.8, 7.9, 8. , 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9. ,\n        9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9]),\n <BarContainer object of 99 artists>)\n\n\n\n\n\n\n# now for each sample of lambda, you must generate a poisson\n# and then you must average those\nimport pandas as pd\npost_pred_dists = pd.DataFrame([make_poisson_pmf(lam, np.arange(10)) \n  for lam in samples[:101] ])\n\n\npost_pred_dists.T\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      ...\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.246598\n      0.301194\n      4.493290e-01\n      0.036964\n      0.045112\n      0.002969\n      0.011302\n      0.030298\n      0.301194\n      0.090736\n      0.082108\n      0.100273\n      0.122465\n      0.272532\n      4.065697e-01\n      0.135342\n      0.332871\n      0.122465\n      0.090736\n      0.135342\n      0.165302\n      0.201898\n      0.049842\n      0.074301\n      0.082108\n      0.100273\n      0.090736\n      0.010252\n      0.015164\n      4.065697e-01\n      0.182686\n      0.055070\n      0.110814\n      0.030298\n      0.110814\n      0.005222\n      0.110814\n      0.030298\n      0.149573\n      0.272532\n      ...\n      0.055070\n      0.074301\n      0.272532\n      0.049842\n      0.022501\n      0.272532\n      0.165302\n      0.036964\n      0.055070\n      0.045112\n      0.036964\n      0.301194\n      0.122465\n      0.122465\n      0.022501\n      0.049842\n      0.182686\n      0.055070\n      0.082108\n      0.165302\n      0.018466\n      0.165302\n      0.055070\n      0.067239\n      0.272532\n      0.100273\n      0.036964\n      0.246598\n      0.006959\n      0.030298\n      0.045112\n      0.082108\n      4.965853e-01\n      0.040834\n      0.040834\n      0.036964\n      0.223131\n      0.033464\n      0.009302\n      0.201898\n    \n    \n      1\n      0.345237\n      0.361433\n      3.594632e-01\n      0.121982\n      0.139848\n      0.017515\n      0.050860\n      0.106042\n      0.361433\n      0.217767\n      0.205269\n      0.230629\n      0.257176\n      0.354292\n      3.659127e-01\n      0.270683\n      0.366158\n      0.257176\n      0.217767\n      0.270683\n      0.297544\n      0.323037\n      0.149526\n      0.193184\n      0.205269\n      0.230629\n      0.217767\n      0.047159\n      0.063690\n      3.659127e-01\n      0.310566\n      0.159704\n      0.243792\n      0.106042\n      0.243792\n      0.027675\n      0.243792\n      0.106042\n      0.284189\n      0.354292\n      ...\n      0.159704\n      0.193184\n      0.354292\n      0.149526\n      0.085505\n      0.354292\n      0.297544\n      0.121982\n      0.159704\n      0.139848\n      0.121982\n      0.361433\n      0.257176\n      0.257176\n      0.085505\n      0.149526\n      0.310566\n      0.159704\n      0.205269\n      0.297544\n      0.073863\n      0.297544\n      0.159704\n      0.181546\n      0.354292\n      0.230629\n      0.121982\n      0.345237\n      0.034797\n      0.106042\n      0.139848\n      0.205269\n      3.476097e-01\n      0.130669\n      0.130669\n      0.121982\n      0.334697\n      0.113777\n      0.043719\n      0.323037\n    \n    \n      2\n      0.241666\n      0.216860\n      1.437853e-01\n      0.201271\n      0.216765\n      0.051670\n      0.114435\n      0.185574\n      0.216860\n      0.261320\n      0.256587\n      0.265223\n      0.270035\n      0.230290\n      1.646607e-01\n      0.270683\n      0.201387\n      0.270035\n      0.261320\n      0.270683\n      0.267789\n      0.258429\n      0.224289\n      0.251139\n      0.256587\n      0.265223\n      0.261320\n      0.108466\n      0.133749\n      1.646607e-01\n      0.263981\n      0.231571\n      0.268171\n      0.185574\n      0.268171\n      0.073338\n      0.268171\n      0.185574\n      0.269980\n      0.230290\n      ...\n      0.231571\n      0.251139\n      0.230290\n      0.224289\n      0.162459\n      0.230290\n      0.267789\n      0.201271\n      0.231571\n      0.216765\n      0.201271\n      0.216860\n      0.270035\n      0.270035\n      0.162459\n      0.224289\n      0.263981\n      0.231571\n      0.256587\n      0.267789\n      0.147726\n      0.267789\n      0.231571\n      0.245087\n      0.230290\n      0.265223\n      0.201271\n      0.241666\n      0.086993\n      0.185574\n      0.216765\n      0.256587\n      1.216634e-01\n      0.209071\n      0.209071\n      0.201271\n      0.251022\n      0.193421\n      0.102739\n      0.258429\n    \n    \n      3\n      0.112777\n      0.086744\n      3.834274e-02\n      0.221398\n      0.223991\n      0.101617\n      0.171652\n      0.216503\n      0.086744\n      0.209056\n      0.213822\n      0.203337\n      0.189025\n      0.099792\n      4.939822e-02\n      0.180455\n      0.073842\n      0.189025\n      0.209056\n      0.180455\n      0.160674\n      0.137829\n      0.224289\n      0.217654\n      0.213822\n      0.203337\n      0.209056\n      0.166315\n      0.187249\n      4.939822e-02\n      0.149589\n      0.223852\n      0.196659\n      0.216503\n      0.196659\n      0.129564\n      0.196659\n      0.216503\n      0.170987\n      0.099792\n      ...\n      0.223852\n      0.217654\n      0.099792\n      0.224289\n      0.205782\n      0.099792\n      0.160674\n      0.221398\n      0.223852\n      0.223991\n      0.221398\n      0.086744\n      0.189025\n      0.189025\n      0.205782\n      0.224289\n      0.149589\n      0.223852\n      0.213822\n      0.160674\n      0.196969\n      0.160674\n      0.223852\n      0.220578\n      0.099792\n      0.203337\n      0.221398\n      0.112777\n      0.144989\n      0.216503\n      0.223991\n      0.213822\n      2.838813e-02\n      0.223009\n      0.223009\n      0.221398\n      0.125511\n      0.219211\n      0.160957\n      0.137829\n    \n    \n      4\n      0.039472\n      0.026023\n      7.668548e-03\n      0.182653\n      0.173593\n      0.149885\n      0.193108\n      0.189440\n      0.026023\n      0.125434\n      0.133639\n      0.116919\n      0.099238\n      0.032432\n      1.111460e-02\n      0.090228\n      0.020307\n      0.099238\n      0.125434\n      0.090228\n      0.072303\n      0.055132\n      0.168217\n      0.141475\n      0.133639\n      0.116919\n      0.125434\n      0.191263\n      0.196611\n      1.111460e-02\n      0.063575\n      0.162293\n      0.108162\n      0.189440\n      0.108162\n      0.171672\n      0.108162\n      0.189440\n      0.081219\n      0.032432\n      ...\n      0.162293\n      0.141475\n      0.032432\n      0.168217\n      0.195492\n      0.032432\n      0.072303\n      0.182653\n      0.162293\n      0.173593\n      0.182653\n      0.026023\n      0.099238\n      0.099238\n      0.195492\n      0.168217\n      0.063575\n      0.162293\n      0.133639\n      0.072303\n      0.196969\n      0.072303\n      0.162293\n      0.148890\n      0.032432\n      0.116919\n      0.182653\n      0.039472\n      0.181236\n      0.189440\n      0.173593\n      0.133639\n      4.967922e-03\n      0.178407\n      0.178407\n      0.182653\n      0.047067\n      0.186329\n      0.189125\n      0.055132\n    \n    \n      5\n      0.011052\n      0.006246\n      1.226968e-03\n      0.120551\n      0.107627\n      0.176864\n      0.173798\n      0.132608\n      0.006246\n      0.060208\n      0.066819\n      0.053783\n      0.041680\n      0.008432\n      2.000628e-03\n      0.036091\n      0.004467\n      0.041680\n      0.060208\n      0.036091\n      0.026029\n      0.017642\n      0.100930\n      0.073567\n      0.066819\n      0.053783\n      0.060208\n      0.175962\n      0.165154\n      2.000628e-03\n      0.021616\n      0.094130\n      0.047591\n      0.132608\n      0.047591\n      0.181972\n      0.047591\n      0.132608\n      0.030863\n      0.008432\n      ...\n      0.094130\n      0.073567\n      0.008432\n      0.100930\n      0.148574\n      0.008432\n      0.026029\n      0.120551\n      0.094130\n      0.107627\n      0.120551\n      0.006246\n      0.041680\n      0.041680\n      0.148574\n      0.100930\n      0.021616\n      0.094130\n      0.066819\n      0.026029\n      0.157575\n      0.026029\n      0.094130\n      0.080401\n      0.008432\n      0.053783\n      0.120551\n      0.011052\n      0.181236\n      0.132608\n      0.107627\n      0.066819\n      6.955091e-04\n      0.114181\n      0.114181\n      0.120551\n      0.014120\n      0.126704\n      0.177777\n      0.017642\n    \n    \n      6\n      0.002579\n      0.001249\n      1.635957e-04\n      0.066303\n      0.055608\n      0.173916\n      0.130348\n      0.077355\n      0.001249\n      0.024083\n      0.027841\n      0.020617\n      0.014588\n      0.001827\n      3.000942e-04\n      0.012030\n      0.000819\n      0.014588\n      0.024083\n      0.012030\n      0.007809\n      0.004705\n      0.050465\n      0.031879\n      0.027841\n      0.020617\n      0.024083\n      0.134904\n      0.115607\n      3.000942e-04\n      0.006124\n      0.045496\n      0.017450\n      0.077355\n      0.017450\n      0.160742\n      0.017450\n      0.077355\n      0.009773\n      0.001827\n      ...\n      0.045496\n      0.031879\n      0.001827\n      0.050465\n      0.094097\n      0.001827\n      0.007809\n      0.066303\n      0.045496\n      0.055608\n      0.066303\n      0.001249\n      0.014588\n      0.014588\n      0.094097\n      0.050465\n      0.006124\n      0.045496\n      0.027841\n      0.007809\n      0.105050\n      0.007809\n      0.045496\n      0.036180\n      0.001827\n      0.020617\n      0.066303\n      0.002579\n      0.151030\n      0.077355\n      0.055608\n      0.027841\n      8.114273e-05\n      0.060896\n      0.060896\n      0.066303\n      0.003530\n      0.071799\n      0.139259\n      0.004705\n    \n    \n      7\n      0.000516\n      0.000214\n      1.869665e-05\n      0.031257\n      0.024626\n      0.146587\n      0.083795\n      0.038677\n      0.000214\n      0.008257\n      0.009943\n      0.006774\n      0.004376\n      0.000339\n      3.858353e-05\n      0.003437\n      0.000129\n      0.004376\n      0.008257\n      0.003437\n      0.002008\n      0.001075\n      0.021628\n      0.011841\n      0.009943\n      0.006774\n      0.008257\n      0.088651\n      0.069364\n      3.858353e-05\n      0.001487\n      0.018848\n      0.005484\n      0.038677\n      0.005484\n      0.121705\n      0.005484\n      0.038677\n      0.002653\n      0.000339\n      ...\n      0.018848\n      0.011841\n      0.000339\n      0.021628\n      0.051081\n      0.000339\n      0.002008\n      0.031257\n      0.018848\n      0.024626\n      0.031257\n      0.000214\n      0.004376\n      0.004376\n      0.051081\n      0.021628\n      0.001487\n      0.018848\n      0.009943\n      0.002008\n      0.060029\n      0.002008\n      0.018848\n      0.013955\n      0.000339\n      0.006774\n      0.031257\n      0.000516\n      0.107878\n      0.038677\n      0.024626\n      0.009943\n      8.114273e-06\n      0.027838\n      0.027838\n      0.031257\n      0.000756\n      0.034874\n      0.093502\n      0.001075\n    \n    \n      8\n      0.000090\n      0.000032\n      1.869665e-06\n      0.012894\n      0.009543\n      0.108108\n      0.047135\n      0.016921\n      0.000032\n      0.002477\n      0.003107\n      0.001948\n      0.001149\n      0.000055\n      4.340648e-06\n      0.000859\n      0.000018\n      0.001149\n      0.002477\n      0.000859\n      0.000452\n      0.000215\n      0.008110\n      0.003848\n      0.003107\n      0.001948\n      0.002477\n      0.050974\n      0.036416\n      4.340648e-06\n      0.000316\n      0.006833\n      0.001508\n      0.016921\n      0.001508\n      0.080629\n      0.001508\n      0.016921\n      0.000630\n      0.000055\n      ...\n      0.006833\n      0.003848\n      0.000055\n      0.008110\n      0.024264\n      0.000055\n      0.000452\n      0.012894\n      0.006833\n      0.009543\n      0.012894\n      0.000032\n      0.001149\n      0.001149\n      0.024264\n      0.008110\n      0.000316\n      0.006833\n      0.003107\n      0.000452\n      0.030014\n      0.000452\n      0.006833\n      0.004710\n      0.000055\n      0.001948\n      0.012894\n      0.000090\n      0.067424\n      0.016921\n      0.009543\n      0.003107\n      7.099989e-07\n      0.011135\n      0.011135\n      0.012894\n      0.000142\n      0.014821\n      0.054933\n      0.000215\n    \n    \n      9\n      0.000014\n      0.000004\n      1.661924e-07\n      0.004728\n      0.003287\n      0.070871\n      0.023567\n      0.006581\n      0.000004\n      0.000661\n      0.000863\n      0.000498\n      0.000268\n      0.000008\n      4.340648e-07\n      0.000191\n      0.000002\n      0.000268\n      0.000661\n      0.000191\n      0.000090\n      0.000038\n      0.002703\n      0.001112\n      0.000863\n      0.000498\n      0.000661\n      0.026054\n      0.016994\n      4.340648e-07\n      0.000060\n      0.002202\n      0.000369\n      0.006581\n      0.000369\n      0.047482\n      0.000369\n      0.006581\n      0.000133\n      0.000008\n      ...\n      0.002202\n      0.001112\n      0.000008\n      0.002703\n      0.010245\n      0.000008\n      0.000090\n      0.004728\n      0.002202\n      0.003287\n      0.004728\n      0.000004\n      0.000268\n      0.000268\n      0.010245\n      0.002703\n      0.000060\n      0.002202\n      0.000863\n      0.000090\n      0.013340\n      0.000090\n      0.002202\n      0.001413\n      0.000008\n      0.000498\n      0.004728\n      0.000014\n      0.037458\n      0.006581\n      0.003287\n      0.000863\n      5.522213e-08\n      0.003959\n      0.003959\n      0.004728\n      0.000024\n      0.005599\n      0.028687\n      0.000038\n    \n  \n\n10 rows × 101 columns\n\n\n\n\n(post_pred_dists.T * np.array(france)).sum(axis=1)\n\n0    0.006444\n1    0.011323\n2    0.011493\n3    0.008816\n4    0.005671\n5    0.003244\n6    0.001715\n7    0.000856\n8    0.000408\n9    0.000186\ndtype: float64"
  },
  {
    "objectID": "vignettes/uclh-working-with-star.html",
    "href": "vignettes/uclh-working-with-star.html",
    "title": "HyStakes",
    "section": "",
    "text": "A template JupyterNotebook for working with EMAP. The following features of this notebook, and associated files are documented here to minimise the risk of data leaks or other incidents.\n\nUsernames and passwords are stored in a .env file that is excluded from version control. An example file is found at ./.env.example (which is tracked and shared via version control).\n.gitattributes are set to strip JupyterNotebook cells when pushing to GitHub\n\nNOTE\nThis notebook will be be run manually.\nThat should happen from the project root directory where the readme.md and the .env file are stored.\nThe following changes to the project root assuming that the notebook kernel is normally starting from the same directory as the notebook itself.\n\n%cd ../..\n\n\n\nLoad libraries\n\nimport os\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n\n\n\nLoad environment variables and set-up SQLAlchemy connection engine for the EMAP Star\n\nload_dotenv(dotenv_path=\".env\")\ntry:\n    assert os.getenv(\"DOTENV_FILE_EXISTS\") == 'TRUE'\nexcept AssertionError:\n    print(\"!!! ERROR: check that the .env file exists at the top level of the project\")\n    print(\"!!! ERROR: check that the relative path is correct\")\n\n\n# Construct the PostgreSQL connection\nuds_host = os.getenv('EMAP_DB_HOST')\nuds_name = os.getenv('EMAP_DB_NAME')\nuds_port = os.getenv('EMAP_DB_PORT')\nuds_user = os.getenv('EMAP_DB_USER')\nuds_passwd = os.getenv('EMAP_DB_PASSWORD')\n\nemapdb_engine = create_engine(f'postgresql://{uds_user}:{uds_passwd}@{uds_host}:{uds_port}/{uds_name}')\n\n\n\n\nNow use the connection to work with EMAP.\nFor example, let’s inspect patients currently in ED or Resus.\nHere’s the SQL:\n-- Example script \n-- to pick out patients currently in A&E resus or majors\n\nSELECT\n   vd.location_visit_id\n  ,vd.hospital_visit_id\n  ,vd.location_id\n  -- ugly HL7 location string \n  ,lo.location_string\n  -- time admitted to that bed/theatre/scan etc.\n  ,vd.admission_datetime\n  -- time discharged from that bed\n  ,vd.discharge_datetime\n\nFROM star.location_visit vd\n-- location label\nINNER JOIN star.location lo ON vd.location_id = lo.location_id\nWHERE \n-- last few hours\nvd.admission_datetime > NOW() - '12 HOURS'::INTERVAL    \n-- just CURRENT patients\nAND\nvd.discharge_datetime IS NULL\n-- filter out just ED and Resus or Majors\nAND\n-- unpacking the HL7 string formatted as \n-- Department^Ward^Bed string\nSPLIT_PART(lo.location_string,'^',1) = 'ED'\nAND\nSPLIT_PART(lo.location_string,'^',2) ~ '(RESUS|MAJORS)'\n-- sort\nORDER BY lo.location_string\n;\nThe SQL script is stored at ./utils/queries/current_bed.sql.\nWe can load the script, and read the results into a Pandas dataframe.\n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('utils/queries/current_bed.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\n\ndf.head()\n\n\n\n\nA series of three scripts\n\nSimply pull hospital visits\nAdd in hospital numbers (MRN) and handle patient merges\nAdd in patient demographics\n\n\n\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  -- admission to hospital\n  ,vo.admission_datetime\n  ,vo.arrival_method\n  ,vo.presentation_datetime\n  -- discharge from hospital\n  -- NB: Outpatients have admission events but not discharge events\n  ,vo.discharge_datetime\n  ,vo.discharge_disposition\n\n-- start from hospital visits\nFROM star.hospital_visit vo\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_datetime > NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_datetime DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('./utils/queries/hospital_visit_1.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\n\n\nSee the series of joins in the middle of the script that retrieve the live MRN. That is we recognise that patients may have had an episode of care with one MRN, and then that episode was merged with another historical MRN. One of those two MRNs will then become the ‘live’ MRN and can be used to trace the patient across what otherwise would be different identities.\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  ,vo.admission_datetime\n  ,vo.arrival_method\n  ,vo.presentation_datetime\n  ,vo.discharge_datetime\n  ,vo.discharge_disposition\n  -- original MRN\n  ,original_mrn.mrn AS original_mrn\n  -- live MRN\n  ,live_mrn.mrn AS live_mrn\n\n-- start from hospital visits\nFROM star.hospital_visit vo\n-- get original mrn\nINNER JOIN star.mrn original_mrn ON vo.mrn_id = original_mrn.mrn_id\n-- get mrn to live mapping \nINNER JOIN star.mrn_to_live mtl ON vo.mrn_id = mtl.mrn_id \n-- get live mrn \nINNER JOIN star.mrn live_mrn ON mtl.live_mrn_id = live_mrn.mrn_id \n\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_datetime > NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_datetime DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('./utils/queries/hospital_visit_2.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\n\n\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  ,vo.admission_datetime\n  ,vo.arrival_method\n  ,vo.presentation_datetime\n  ,vo.discharge_datetime\n  ,vo.discharge_disposition\n  -- original MRN\n  ,original_mrn.mrn AS original_mrn\n  -- live MRN\n  ,live_mrn.mrn AS live_mrn\n\n  -- core demographics\n  ,cd.date_of_birth\n  -- convert dob to age in years\n  ,date_part('year', AGE(cd.date_of_birth)) AS age\n  ,cd.sex\n  ,cd.home_postcode\n  -- grab initials from first and last name\n  ,CONCAT(LEFT(cd.firstname, 1), LEFT(cd.lastname, 1)) AS initials\n\n-- start from hospital visits\nFROM star.hospital_visit vo\nINNER JOIN star.core_demographic cd ON vo.mrn_id = cd.mrn_id\n\n-- get original mrn\nINNER JOIN star.mrn original_mrn ON vo.mrn_id = original_mrn.mrn_id\n-- get mrn to live mapping \nINNER JOIN star.mrn_to_live mtl ON vo.mrn_id = mtl.mrn_id \n-- get live mrn \nINNER JOIN star.mrn live_mrn ON mtl.live_mrn_id = live_mrn.mrn_id \n\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_datetime > NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_datetime DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('./utils/queries/hospital_visit_3.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()"
  },
  {
    "objectID": "vignettes/how-to-count.html",
    "href": "vignettes/how-to-count.html",
    "title": "Counting deaths",
    "section": "",
    "text": "There’s a nice visual explanation here.\nWe observe the outcomes of a series of binary choices: dead or alive, left of right, male or female, or healthy or diseased. We can imagine a series of these choices as a garden of forking paths. Let’s work with counts of deaths (mortality) and label them as died (D) or survived (S).\n\nThe first fork: there are just two possible outcomes: D or S.\nThe second fork is identical but replicated for each of the outcomes leading from the first fork: DD , or DS, or SD, or SS. You can read these as case 1 and case 2 died (DD), or case 1 died but case 2 survived (DS) etc.\nThe third fork produces eight possible series: DDD, DDS, DSD, DSS, SDD, SDS, SSD, and SSS.\n\nAnd so on.\nBut we are interested in the count of deaths not the sequence so at the second fork DS is equivalent to SD: one death, one survivor each. So if the order doesn’t matter then there is one path to 2 deaths, two paths to 1 death, and one path to 2 survivors (0 deaths). At the third fork, there is also one path to 3 deaths, three paths to 2 deaths, three paths to 1 death, and one path to zero deaths.\nIf we want to go beyond counting by hand, we need to see if we can find a mathematical trick to make the counting easier. Pascal found one in his triangle.\n\nBut even writing out such a triangle is not going to be sustainable.\nLet’s rephrase the problem, and start from the sequences we generate at the ends of the paths. For any particular number of deaths (i), split the problem into two parts. First count how many ways you can choose i deaths from a group of n subjects. Concretely, choose 2 deaths among 3 patients: you have 3 choices for the first death, and then 2 choices for the second death: \\(3\\times2\\). Second, count how many ways you can order those i deaths: there are 2 choices for the first death, and just 1 choice for the second death: \\(2\\times1\\). Since you only care about the count not the ordering then you divide the count of choices by the count of orderings.\n\n3 deaths in 3 subjects.\n\nThere are 3 choices for the first death, 2 choices for the second death, and 1 choice for the last death: \\(3\\times2\\times1\\)\nThere are 3 orderings of deaths: \\(3\\times2\\times1\\)\nSo there is just 1 way \\(\\left(\\frac{3\\times2\\times1}{3\\times2\\times1}\\right(\\) of choosing 3 deaths from 3 subjects\n\n2 deaths in 3 subjects\n\nThere are 3 choices for the first death, and 2 choices for the second death: \\(3\\times2\\)\nThere are 2 orderings of deaths: \\(2\\times1\\)\nSo there is are 3 ways \\(\\left(\\frac{3\\times2}{2\\times1}\\right)\\) way of choosing 2 deaths from 3 subjects\n\n1 deaths in 3 subjects\n\nThere are 3 choices for the first death: \\(3\\)\nThere is only 1 ordering of deaths: \\(1\\)\nSo there is are 3 ways \\(\\left(\\frac{3}{1}\\right)\\) way of choosing 1 death from 3 subjects\n\n0 deaths in 3 subjects\n\nBy symmetry, this is the same as choosing 3 survivors in 3 subjects as per (1) above: there is only 1 way\nBy the definition of a factorial \\(0!=1\\) then we can say that zero choices divided by zero orderings is \\(\\left(\\frac{0!}{0!}\\right)\\) is also 1.\n\n\n\nThe example is trivial but the principle of dividing the count of choices by the count of orderings allow us to work with deeper layers Pascal’s triangle, and to count the number of ways that forking paths can produce any particular count.\nConsider our problem of counting the ways 6 deaths can arise among 10 patients. This can now be calculated as\n\\[\n\\begin{aligned}\n    count &= \\frac{\\text{count of choices}}{\\text{count of orderings}} \\\\\n    &= \\frac{10\\times9\\times8\\times7\\times6\\times5}\n                    {6\\times5\\times4\\times3\\times2\\times1} \\\\\n    &= \\frac{5040}{24} \\\\\n    &= 210\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "vignettes/learn-stats-pandemic.html",
    "href": "vignettes/learn-stats-pandemic.html",
    "title": "Learning statistics in a pandemic",
    "section": "",
    "text": "So there’s a global pandemic. Your first task is to report the mortality rate. The government will only be convinced to act if the mortality is greater than that for seasonal flu (approximately 1%).\nYou have a colleague in Wuhan. She struggles to send you data because she’s worried about censorship so you communicate with a simple code using Twitter. If the last character of the tweet is a punctuation mark or emoji that means a case of COVID-19 admitted to her hospital has died. If the last character is not a punctuation mark or emoji, then the patient has been discharged alive.\nHere are the first 3 tweets from her.\n\nHi. How are you?\n\n\nIt’s cold. Say hi to your family!\n\n\nBetter day today, warmer\n\nSo we read this as\n\nDied\nDied\nSurvived\n\nFrom now on, we’re going to represent this as a string of characters DDS. You receive 7 more tweets over the next couple of days: DDSDSSDDDS . That is 6 deaths and 4 survivors, or a 60% mortality rate.\nLet’s recap.\n\nWe have some measurements or observations (our data).\nWe want to know the ‘true’ mortality rate, and are nervous about causing unnecessary alarm.\n\nPoint (2) is crucial because in framing the question, we are acknowledging that we have uncertainty. From our perspective, the true mortality rate should not be reported as a single number (in this case 6/10 or 60%). That might be true, but it excludes our uncertainty. It would be an incomplete description rather like saying the latitude and longitude of London is (0º,0º). That is not even the latitude and longitude of the Greenwich Observatory because both the Observatory and London are not single points in space. They have a spread from north to south and east to west.\nTo capture uncertainty, we must treat the true mortality rate as a range, or distribution, of values. Not all values in the range are equally plausible even before we observe any data. The mortality cannot be less than 0% or greater than 100% so our prior belief (p) lies in the range 0 ≤ p ≤ 1.\nLet’s recap again. We now have two useful tools. 1. Data 2. A prior (belief) about the mortality rate.\nWe will use the data to update our prior, and you will not be surprised to learn that we will call the result the posterior (probability distribution). The more data we have, the more we can pin down our prior. Let’s take the outcomes for the first three patients as an example.\n\nOur initial prior says the risk of death is anywhere from 0 ≤ p ≤ 1.\nThe first and second patients both died. It’s not a lot of information but we can at least say the risk of death cannot be zero. We now believe p lies in the range 0 < p ≤ 1.\nThe third patient survived so similarly we p must now lie in the range 0 < p < 1.\n\nThis discrete logic, ruling in and ruling out certain values based on observations gets us so far. But there is more information still in just these first three cases DDS. We need a tool that connects the count of deaths to our prior. The more data we see, the more important this will become. Three cases becomes ten cases with six deaths (DDSDSSDDDS). But the tweets don’t stop there. When our final sample becomes 1000 cases with 600 deaths, the proportion does not change. But we will be much, much more worried because we have an intuition that the proportion from the bigger sample is a more certain estimate.\nSo how do we connect the observed data to our beliefs. We need to describe a mechanism for generating the data. The mechanism must only be based on things that we can verify, or are happy to assume. So one mechanism might be that a divine being ordered there six deaths in this specific sequence. Could be true but it doesn’t really help move us forwards … A better mechanism is to imagine a series of coin flips with a special coin that is weighted to show tails in the same proportion that deaths occur.\nsee How to count\nThis model creates a connection between the data and the mortality rate, and it does so with the minimum of prior assumptions (see maximum information entropy). For example, imagine that we knew the mortality rate was 50%. The model tells us how that data would have to come about. (show 10th level of forking paths, and highlight the set of paths that arrive at 6 deaths). More importantly, the model also shows all the other data that could have been generated from that same assumed mortality rate. We see that many more paths arrive at 6 than 1 or 2 or 3 death, for example. But we also note that 5 deaths is the commonest finding. Our intuition tells us that our data (our observation) provide quite good evidence for a mortality rate of 50%. but very weak evidence for mortality rates of 10%, 20% or 80%, 90% etc.\nIf we now vary our assumed mortality rate across the range of our prior belief then we can repeat this exercise and build up a picture of how the data has updated our prior as assumptions: the posterior.\nWe could just report the proportion of deaths in the sample we have observed. We’d want to also report the number of observations (i.e. the ‘sample size’ ). We’d be a lot more worried if this was a sample of 1000 cases a not just 10. But why? We have an intuition that the proportion from the bigger sample (600 deaths in 1000 cases) is a more certain estimate.\nBut again why?\nTo answer this, we need to connect our data with the ‘answer to our question’ (read target of inference) with a story (read model). The model is a data generating mechanism that in some way depends on the true proportion of deaths among patients with COVID-19.\n\nYou … motivate your data story by trying to explain how each piece of data is born.\n\nLet’s go back to the first 3 patients (DDS), and invoke a unfair but divine coin labelled with D on one side, and S on the other. It is flipped for each victim, and for now, we are going to assume that the same coin is used for everyone. The ‘chance’ of the coin landing D side up will be called p. This is our target of inference. It is an unseen variable. The data is summarised as two ‘observed’ variables: the count of D and the count of S, or equivalent the count of D and the total number (N) of patients (since the coin has only 2 sides these representations are equivalent.).\nThe data story is a probability model. It counts the number of ways a particular observation can come about based on the parameters of the model. In doing so, it links the parameter with the data.\nConcretely, here, we have a probability model based on a coin-toss. The parameter is the ‘fairness’ of the coin. And we want to learn from the observed data how ‘fair’ the coin is."
  },
  {
    "objectID": "vignettes/vignettes.html",
    "href": "vignettes/vignettes.html",
    "title": "Vignettes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\nExample Jupyter Notebook\n\n\n\n\n\n\n\n\nPrepare synthetic data\n\n\n\n\n\n\n\n\nPyMC set-up\n\n\n\n\n\n\n\n\nChapter 8. Poisson Processes\n\n\n\n\n\n\n\n\nWorking with EMAP star\n\n\n\n\n\n\nOct 18, 2022\n\n\nEstimating discharge probability\n\n\nSteve Harris\n\n\n\n\nMay 25, 2020\n\n\nCounting deaths\n\n\nSteve Harris\n\n\n\n\nMay 24, 2020\n\n\nLearning statistics in a pandemic\n\n\nSteve Harris\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorial/bayes-for-discharge.html",
    "href": "tutorial/bayes-for-discharge.html",
    "title": "Predicting discharge using Bayesian statistics",
    "section": "",
    "text": "Note that, for now in this first attempt, I’m assuming you have some prior knowledge of Bayesian statistics.\nHere are three initial steps I think we need to do:\n\nDecide on our “prior” probability and express it as a distribution\nIdentify some data we can use for the “likelihood”. This can be synthetic to start with\nDevelop a posterior distribution from the prior and likelihood.\n\n\n1. Prior probability\nWe express the probability of discharge as \\(\\theta\\). Our prior belief about \\(\\theta\\) could be that it follows a beta distribution which is expressed as follows:\n\\[\nP(\\theta) = Beta(\\alpha,\\beta)\n\\tag{1}\\]\nLet’s say we know that 5% of patients are discharged on any given day. Therefore we’d like the expected value of our Beta distribution to be at 0.05. The expected value of a Beta distribution is given by\n\\[\nExp(\\theta) = \\frac{\\alpha}{\\alpha + \\beta}\n\\]\nIf we set \\(\\alpha\\) to be 2 and \\(\\beta\\) to be 38, we’ll get an expected value of 0.05\n\\[\nP(\\theta) = Beta(2,38)\n\\tag{2}\\]\nPlotting this:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n\na, b = 2, 38\nfig, ax = plt.subplots(1, 1)\n\ntheta = np.linspace(0, 1, 100)\nax.plot(theta, beta.pdf(theta, a, b),\n       'r-', lw=2, alpha=0.6, label='beta pdf')\nax.set_ylabel('probability density function')\nax.set_xlabel('theta = probability of discharge in 24 hours')\nax.set_title('Prior belief about distribution of theta')\nplt.xlim([0, 1])\n\nplt.show()\n\n\n\n\n\nPrior distribution\n\n\n\n\n\n\n2. Identify some data we can use for the “likelihood”\nNow let’s look at some data. This will constitute our likelihood. First let’s read it in from a saved anonymised database\n\n\nCode\nimport sqlalchemy as sa\nimport pandas as pd\nimport os\n\n\nsqlite_engine = sa.create_engine('sqlite:///../../data/dummy.db')\ndf = pd.read_sql_query(\"SELECT * from discharges\", sqlite_engine)\ndf.head()\n\n# Let N be the number of patients observed\nN = df.shape[0]\n\n# Let X be the number of patients who were discharged in 24 hours\nX = df[df.hours_to_discharge <= 24].shape[0]\nprint(\"Number of people discharged in 24 hours: \" + str(X))\nprint(\"Total number of people observed: \" + str(N))\n\n\nNumber of people discharged in 24 hours: 52\nTotal number of people observed: 444\n\n\nWe observe that 52 patients out of 444 were discharged. This provides our likelihood. We can express our data as a binomial distribution where we do N trials with our unknown probability \\(\\theta\\) and return the result we are looking for (ie patient was discharged) X times of the N trials.\n\\[\nP(data) = Binomial(N, \\theta)\n\\tag{3}\\]\nThe binomial distribution can be expanded as shown here, where \\({N \\choose X}\\) is the notation for N Choose X (a way of expressing the number of different ways, or permutations, that you could get a result of X discharged patients when starting with N patients in total)\n\\[\nP(data) = {N \\choose X} \\theta^x(1-\\theta)^{(N-X)}\n\\]\nWe can plot the likelihood\n\n\nCode\nimport pandas as pd\nimport os\n\n#| label: likelihood\n#| fig-cap: \"Likelihood distribution\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\nfrom math import factorial as fact\nN_choose_X = fact(N)/(fact(X)*fact(N-X))\n\nfig, ax = plt.subplots(1, 1)\n\n\nline1, = ax.plot(theta, beta.pdf(theta, a, b),\n       'r-', lw=2, alpha=0.6, label='prior')\nline2, = ax.plot(theta, N_choose_X*theta**X*(1-theta)**(N-X)*N,\n       'g-', lw=2, alpha=0.6, label='likelihood')\nax.legend(handles=[line1, line2])\nax.set_ylabel('probability density function')\nax.set_xlabel('theta = probability of discharge in 24 hours')\nax.set_title('Prior belief about distribution of theta, and likelihood of data')\n\n\nplt.show()\n\n\n\n\n\nNow that we have seen the data, we see that our original prior was too pessimistic. A higher proportion of patients are discharged each day than we expected.\n\n\n3. Develop a posterior distribution from the prior and likelihood\nWe know from Bayesian statistics that, if you have a beta prior, and binomial likelihood then you can derive the posterior distribution from these without doing any complicated maths.\nSpecifically, our posterior can be expressed as a beta distribution, in which we can bring in the values of \\(\\theta\\) from our prior, and values of N and X from our likelihood\n\\[\nP(\\theta | data) = Beta(\\alpha + X,N + \\beta - X)\n\\]\nAnd this can also be expressed as\n\\[\nP(\\theta | data) = \\frac{\\theta^{(\\alpha + X -1)}(1 - \\theta)^{N + \\beta - X -1}}{B (\\alpha + X -1, N + \\beta - X -1)}\n\\] The denominator above is referred to as a normalising constant. It looks confusing, but the key thing to note that it doesn’t contain any values of \\(\\theta\\), making it simpler to work out than when you have to integrate over all values of \\(\\theta\\).\nWe can now draw our posterior distribution (in blue)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n\na_prime, b_prime = 2 + X , 38 + N - X\nfig, ax = plt.subplots(1, 1)\n\n\nline1, = ax.plot(theta, beta.pdf(theta, a, b),\n       'r-', lw=2, alpha=0.6, label='prior')\nline2, = ax.plot(theta, N_choose_X*theta**X*(1-theta)**(N-X)*N,\n       'g-', lw=2, alpha=0.6, label='likelihood')\nline3, = ax.plot(theta, beta.pdf(theta, a_prime, b_prime),\n       'b-', lw=2, alpha=0.6, label='posterior')\nax.legend(handles=[line1, line2, line3])\n\n\nplt.show()\n\n\n\n\n\nPosterior distribution\n\n\n\n\nThe posterior predictive distribution is in a tighter range (a taller, slimmer peak) than the prior predictive distribution, and is overwhelmingly determined by the data, rather than the prior. This is the case with Bayesian statistics: as you get more data, the prior beliefs become less important."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HyStakes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "HyStakes",
    "section": "Section 1",
    "text": "Section 1\nsome text"
  },
  {
    "objectID": "index.html#section-2",
    "href": "index.html#section-2",
    "title": "HyStakes",
    "section": "Section 2",
    "text": "Section 2\nsome more text"
  }
]